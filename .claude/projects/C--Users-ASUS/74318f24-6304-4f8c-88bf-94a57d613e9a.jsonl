{"type":"file-history-snapshot","messageId":"346bb9a5-df8f-4d2d-bc60-1e71be2e4eae","snapshot":{"messageId":"346bb9a5-df8f-4d2d-bc60-1e71be2e4eae","trackedFileBackups":{},"timestamp":"2026-02-20T17:39:28.327Z"},"isSnapshotUpdate":false}
{"parentUuid":null,"isSidechain":false,"userType":"external","cwd":"C:\\Users\\ASUS","sessionId":"74318f24-6304-4f8c-88bf-94a57d613e9a","version":"2.1.47","gitBranch":"master","type":"user","message":{"role":"user","content":"C:\\Users\\ASUS\\Desktop\\h4h-2026-championship-plan.md You are going to read this file and understand how this project does. You are going to answers my question. Here is the question: for photo uploading, do we upload photos on mobile app or directly on Apple Vision Pro, or other method? DO not change the contents in that file. Tell me the answer in this chatbot. "},"uuid":"346bb9a5-df8f-4d2d-bc60-1e71be2e4eae","timestamp":"2026-02-20T17:39:28.323Z","todos":[],"permissionMode":"default"}
{"parentUuid":"346bb9a5-df8f-4d2d-bc60-1e71be2e4eae","isSidechain":false,"userType":"external","cwd":"C:\\Users\\ASUS","sessionId":"74318f24-6304-4f8c-88bf-94a57d613e9a","version":"2.1.47","gitBranch":"master","message":{"model":"claude-sonnet-4-6","id":"msg_01E2cq7ckQeAHrqtLq5MBahL","type":"message","role":"assistant","content":[{"type":"thinking","thinking":"The user wants me to read the file at C:\\Users\\ASUS\\Desktop\\h4h-2026-championship-plan.md and answer a specific question about photo uploading. Let me read it.","signature":"EsgCCkYICxgCKkCOuoyLQ5mopiXGZHxtITC/ZtJ8aX0Kj01q1vylj65fe281Z9SFyNTPDqfBWSZVJeo0vpZEFJ56aYQb674AGa/aEgy/eBO+CYzlDnQolcMaDPjaHf7iIKR0FPGeFCIwwisu8vKQagRAouEiNKtCzfALibpmsjoVUNe/c4Fn0fUYWW01aPvY3rMhGHGWeQZkKq8B9BguZUnm6vfjDt8ZqVEewpdZeBkgRX24emTFqDGlY5EjgEV4xGiDx0c3j/kwPcFDq8o6XOlkHbJNrOtJZYqlsv8Ex9EeM0QmHjqfJSKQ0asJyraH8pZYNGhkd7HPEYtkx+hmuhbYaBi0Bipob1XdfGd+JDIu/mizOavokupCTLPfaoYdIOoimPPyb9jfgoLwMz1/6H3ykVjSW9HLBa+n3/hsgVqoKdP9AY7/+KZO7BgB"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":28120,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":28120},"output_tokens":6,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYKjHtzrYaD2koobU7qtk","type":"assistant","uuid":"660cac3c-1721-4297-b692-f23c9ede74ba","timestamp":"2026-02-20T17:39:31.350Z"}
{"parentUuid":"660cac3c-1721-4297-b692-f23c9ede74ba","isSidechain":false,"userType":"external","cwd":"C:\\Users\\ASUS","sessionId":"74318f24-6304-4f8c-88bf-94a57d613e9a","version":"2.1.47","gitBranch":"master","slug":"expressive-juggling-ladybug","message":{"model":"claude-sonnet-4-6","id":"msg_01E2cq7ckQeAHrqtLq5MBahL","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BXmBFujeMXzGMNpFN6sNis","name":"Read","input":{"file_path":"C:\\Users\\ASUS\\Desktop\\h4h-2026-championship-plan.md"},"caller":{"type":"direct"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":28120,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":28120},"output_tokens":6,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYKjHtzrYaD2koobU7qtk","type":"assistant","uuid":"df4895a9-a453-47f1-819c-03627c1e1a3a","timestamp":"2026-02-20T17:39:31.763Z"}
{"parentUuid":"df4895a9-a453-47f1-819c-03627c1e1a3a","isSidechain":false,"userType":"external","cwd":"C:\\Users\\ASUS","sessionId":"74318f24-6304-4f8c-88bf-94a57d613e9a","version":"2.1.47","gitBranch":"master","slug":"expressive-juggling-ladybug","type":"progress","data":{"type":"hook_progress","hookEvent":"PostToolUse","hookName":"PostToolUse:Read","command":"callback"},"parentToolUseID":"toolu_01BXmBFujeMXzGMNpFN6sNis","toolUseID":"toolu_01BXmBFujeMXzGMNpFN6sNis","timestamp":"2026-02-20T17:39:31.817Z","uuid":"3826429f-0fa3-417f-84e2-ee96fdf824fa"}
{"parentUuid":"df4895a9-a453-47f1-819c-03627c1e1a3a","isSidechain":false,"userType":"external","cwd":"C:\\Users\\ASUS","sessionId":"74318f24-6304-4f8c-88bf-94a57d613e9a","version":"2.1.47","gitBranch":"master","slug":"expressive-juggling-ladybug","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BXmBFujeMXzGMNpFN6sNis","type":"tool_result","content":"     1→# H4H 2026 Championship Plan — MemoryBridge\n     2→\n     3→## THE CHAMPION PROJECT: MemoryBridge\n     4→\n     5→> \"We give dementia patients their own voice back — to narrate their own memories.\"\n     6→\n     7→### Elevator Pitch (30 seconds)\n     8→57 million people worldwide live with dementia. They forget their children's names, their wedding days, their own stories. MemoryBridge lets families upload photos and a 60-second voice recording of their loved one. Our AI clones their voice, builds a semantic memory graph from their life history, and creates an immersive spatial memory room on Apple Vision Pro — where they can walk through their past and hear themselves remember.\n     9→\n    10→---\n    11→\n    12→## PRIZE TARGETING (up to $9,000+)\n    13→\n    14→| Prize | Amount | How We Win It |\n    15→|-------|--------|----------------|\n    16→| Best Freshman Hack | $1,000 | ALL team members are freshmen; innovative + functional + well-designed |\n    17→| Best Project Built with ElevenLabs | $1,980/member | Voice cloning IS the product, not a feature |\n    18→| Best Use of AMD Tech | $1,000 | AMD GPU powers the semantic memory graph + embeddings |\n    19→| Best Use of WebSpatial | $1,000 | Spatial memory room IS the core UX |\n    20→| Future Unicorn Award | $1,000 | $350B eldercare market, clear B2B path |\n    21→| Best Use of Responsible AI | $750 | Preserving dignity in aging, ethical framing |\n    22→| Grand Prize (1st) | $2,250 | Every judge criteria: Tech ✓ Polish ✓ Innovativeness ✓ Social Impact ✓ |\n    23→\n    24→---\n    25→\n    26→## SYSTEM ARCHITECTURE\n    27→\n    28→```\n    29→[Family] ──── React Upload Interface\n    30→                    │\n    31→                    ├── Firebase Storage ──── Photos + Voice Recording\n    32→                    │\n    33→                    └── Flask Backend (Python)\n    34→                              │\n    35→                    ┌─────────┴──────────────┐\n    36→                    │                        │\n    37→             AMD GPU Cloud              ElevenLabs\n    38→         (sentence-transformers      (Instant Voice Clone\n    39→          embeddings → semantic       + Conversational AI\n    40→          memory graph)               Agent with knowledge base)\n    41→                    │                        │\n    42→                    └─────────┬──────────────┘\n    43→                              │\n    44→                    [Patient / Family]\n    45→                              │\n    46→                    WebSpatial (React)\n    47→                    on Apple Vision Pro\n    48→                    ────────────────────\n    49→                    3D Spatial Memory Room:\n    50→                    - Floating photo panels by era\n    51→                    - Patient navigates by gaze + pinch\n    52→                    - Reaches for photo → voice responds\n    53→                    - Cloned voice narrates memories\n    54→                    - Live conversational Q&A\n    55→```\n    56→\n    57→---\n    58→\n    59→## TECH STACK\n    60→\n    61→| Layer | Technology | Why |\n    62→|-------|-----------|-----|\n    63→| Frontend | React + Vite + WebSpatial SDK | Workshop-taught, spatially renderable |\n    64→| Backend | Flask + Python | Workshop-taught, fast to prototype |\n    65→| Database | Firebase Firestore + Storage | Real-time, no-SQL, free tier generous |\n    66→| AI Voice | ElevenLabs Instant Voice Clone + Conversational AI | 15-min hello world, voice IS the product |\n    67→| AI Compute | AMD Developer Cloud (Instinct MI300X) | GPU for embeddings + LLM inference |\n    68→| Spatial | WebSpatial SDK on visionOS (Apple Vision Pro) | ByteDance hardware provided at event |\n    69→| Deployment | Firebase Hosting (frontend) + AMD Cloud (backend) | Both deployable in < 30 min |\n    70→\n    71→---\n    72→\n    73→## PRE-HACKATHON PREP (Feb 17-27) — CRITICAL\n    74→\n    75→### TODAY (Feb 17) — URGENT\n    76→- [ ] **Apply for AMD Developer Cloud account** at devcloud.amd.com (3-BUSINESS-DAY approval!)\n    77→- [ ] **Verify: Does any team member have a Mac with Apple Silicon (M1/M2/M3/M4)?** WebSpatial REQUIRES this\n    78→- [ ] Create ElevenLabs account (free 1-month Creator tier at event, but test now)\n    79→- [ ] Create Firebase project at console.firebase.google.com\n    80→\n    81→### Feb 18-19\n    82→- [ ] Install Xcode + visionOS Simulator (large download — do this on good WiFi)\n    83→- [ ] Run WebSpatial quick example: github.com/webspatial/quick-example\n    84→- [ ] Test ElevenLabs voice clone with a 90-second clean voice recording\n    85→- [ ] Set up Firebase Firestore + Storage rules (test mode)\n    86→\n    87→### Feb 20-22\n    88→- [ ] Set up Flask + Firebase Admin SDK boilerplate\n    89→- [ ] Install sentence-transformers: `pip install sentence-transformers`\n    90→- [ ] Test AMD Developer Cloud account (if approved): run vLLM hello world\n    91→- [ ] Build rough React photo upload component\n    92→\n    93→### Feb 23-25\n    94→- [ ] Integrate WebSpatial into existing React app\n    95→- [ ] Test: photo panel → spatial floating card on visionOS Simulator\n    96→- [ ] Test: ElevenLabs agent with custom knowledge base (upload a test document)\n    97→- [ ] Test: AMD GPU endpoint → embeddings → cosine similarity search\n    98→\n    99→### Feb 26-27 (Day Before)\n   100→- [ ] Full dry run of the complete pipeline (upload → clone → spatial room → conversation)\n   101→- [ ] Prepare DEMO AUDIO: clean 90-120 second voice recording (used during judging)\n   102→- [ ] Prepare demo photo set (20-30 photos with dates/captions)\n   103→- [ ] Team roles locked down\n   104→- [ ] Create GitHub repo (note: must make it PUBLIC after 10AM on 2/28)\n   105→- [ ] Draft Devpost description\n   106→\n   107→---\n   108→\n   109→## TEAM ROLES (2-4 people)\n   110→\n   111→### For 4 People:\n   112→| Role | Person | Owns |\n   113→|------|--------|------|\n   114→| Frontend Lead | Person 1 | React UI, photo upload, memory timeline |\n   115→| Spatial Lead | Person 2 | WebSpatial integration, Vision Pro UX |\n   116→| Voice/AI Lead | Person 3 | ElevenLabs clone + Conversational agent |\n   117→| Backend Lead | Person 4 | Flask, Firebase, AMD GPU embeddings |\n   118→\n   119→### For 2 People:\n   120→| Role | Person | Owns |\n   121→|------|--------|------|\n   122→| Frontend + Spatial | Person 1 | React + WebSpatial + ElevenLabs widget |\n   123→| Backend + AI | Person 2 | Flask + Firebase + AMD + ElevenLabs agent |\n   124→\n   125→---\n   126→\n   127→## HOUR-BY-HOUR BUILD PLAN (22 Hours)\n   128→\n   129→### Phase 1: Foundation (Hours 0-4, 11AM-3PM Feb 28)\n   130→\n   131→**11:00 AM — Hacking begins. Create GitHub repo immediately.**\n   132→\n   133→| Time | Person 1 (Frontend) | Person 2 (Backend) | Person 3 (Voice/AI) | Person 4 (Spatial) |\n   134→|------|--------------------|--------------------|--------------------|--------------------|\n   135→| 11-12PM | React + Vite setup | Flask + Firebase setup | ElevenLabs account + first voice clone | Install Xcode + WebSpatial |\n   136→| 12-1PM | Photo upload UI | Firestore schema design | ElevenLabs Conversational agent | WebSpatial hello world |\n   137→| 1-2PM | LUNCH BREAK | LUNCH BREAK | LUNCH BREAK | LUNCH BREAK |\n   138→| 2-3PM | Photo grid UI | Flask → Firestore write | Agent knowledge base test | WebSpatial photo panel |\n   139→\n   140→**3PM: ATTEND AMD WORKSHOP — all hands. Take notes, get AMD account access.**\n   141→\n   142→### Phase 2: Core Features (Hours 4-10, 3PM-9PM Feb 28)\n   143→\n   144→| Time | What Gets Built |\n   145→|------|----------------|\n   146→| 3-4PM | AMD Workshop — everyone attends, note AMD GPU endpoint |\n   147→| 4-5PM | AMD: Flask → AMD GPU → embeddings pipeline working |\n   148→| 4-5PM | ElevenLabs: agent receives knowledge base (photo captions as text docs) |\n   149→| 5-6PM | WebSpatial: multiple spatial panels with photos floating in 3D |\n   150→| 5-6PM | Flask: POST /api/upload → Firebase Storage → trigger embedding job |\n   151→| 6-7PM | DINNER BREAK |\n   152→| 7-8PM | Integration test: upload photo → appears in WebSpatial room |\n   153→| 7-8PM | ElevenLabs agent: can answer \"what happened in 1987?\" from knowledge base |\n   154→| 8-9PM | Voice clone audio: prepare and run voice clone from demo audio |\n   155→| 8-9PM | Present framing workshop attendance: \"How to Present\" at 8PM |\n   156→\n   157→### Phase 3: Integration Sprint (Hours 10-16, 9PM-3AM)\n   158→\n   159→| Time | What Gets Built |\n   160→|------|----------------|\n   161→| 9-10PM | Conversational agent now uses CLONED VOICE (swap voice in ElevenLabs dashboard) |\n   162→| 10-11PM | WebSpatial: clicking/gazing at a photo triggers the voice agent to narrate that memory |\n   163→| 11PM-12AM | Firebase real-time sync: new photos appear in spatial room without refresh |\n   164→| 12-1AM | Polish: memory timeline sorted by era, visual design of spatial room |\n   165→| 1-2AM | AMD semantic graph: visually show memory connections (bonus feature if time) |\n   166→| 2-3AM | Integration testing end-to-end: upload → room → converse → clone voice responds |\n   167→| 3AM | RAMEN BREAK — mandatory rest |\n   168→\n   169→### Phase 4: Polish + Demo Prep (Hours 16-22, 3AM-9AM Mar 1)\n   170→\n   171→| Time | What Gets Built |\n   172→|------|----------------|\n   173→| 3-5AM | Bug fixes, edge cases, loading states |\n   174→| 5-6AM | Devpost submission started (add screenshots, description draft) |\n   175→| 6-7AM | SLEEP (at least 1-2 hours mandatory — do not skip, demo quality tanks) |\n   176→| 7-8AM | Final Devpost writeup + screenshots |\n   177→| 8-8:30AM | Attend Devpost Q&A session |\n   178→| 8:30-9AM | Submit Devpost. Final demo rehearsal. |\n   179→\n   180→**9:00 AM HARD DEADLINE: Devpost submitted.**\n   181→\n   182→---\n   183→\n   184→## DEMO SCRIPT (4 min + 1 min Q&A)\n   185→\n   186→### Opening (30s)\n   187→> \"57 million people live with dementia today. By 2050, that number will triple. They forget their children's names. Their wedding days. The sound of their own voice when they were young. We built MemoryBridge to give that voice back.\"\n   188→\n   189→### Show the Web App (45s)\n   190→Demonstrate the family upload interface — drag in 20 photos with dates/captions, upload a 90-second voice recording. Show the processing screen (\"Building your memory...\").\n   191→\n   192→### The Voice Clone Reveal (30s)\n   193→Play the original voice recording. Then play the ElevenLabs-cloned version. Let the room feel the uncanny similarity.\n   194→> \"That voice will now live inside the memory room forever.\"\n   195→\n   196→### The Spatial Memory Room (60s)\n   197→Put on the Apple Vision Pro (or show screencapture on main display). Navigate through floating photo panels organized by decade. Reach toward the wedding photo — it expands. The cloned voice speaks:\n   198→> \"That was May 3rd, 1974. You wore your mother's dress. It rained in the morning but the sun came out just in time.\"\n   199→\n   200→### Live Conversational Demo (45s)\n   201→Ask the agent out loud: \"Where did we go on our first vacation?\"\n   202→The system responds in the cloned voice with the answer drawn from the uploaded memories.\n   203→> \"This is not a recording. This is a live AI conversation — in the patient's own voice.\"\n   204→\n   205→### The Impact (30s)\n   206→> \"We tested this with two families during building. One caregiver said: 'My father hasn't recognized me in two years. But when I played this, he smiled.' That's why we built MemoryBridge. The eldercare market is $350 billion. We're starting with the most vulnerable people in it.\"\n   207→\n   208→### Technical Close (20s)\n   209→> \"Built on ElevenLabs voice cloning and conversational AI, AMD GPU-accelerated semantic memory graphs, WebSpatial on Apple Vision Pro, and Firebase for real-time family collaboration.\"\n   210→\n   211→---\n   212→\n   213→## RISK MATRIX + MITIGATIONS\n   214→\n   215→| Risk | Probability | Impact | Mitigation |\n   216→|------|-------------|--------|-----------|\n   217→| AMD account not approved | Medium | High | Apply TODAY. Fallback: OpenAI API as LLM, note \"AMD in production\" |\n   218→| No Mac with Apple Silicon | Medium | High | Check NOW. Fallback: WebSpatial web view without Vision Pro (still impressive) |\n   219→| Voice clone sounds robotic | Medium | Critical | Use 90-120s clean audio. Pre-test Feb 26. Backup: use ElevenLabs' \"Aria\" voice |\n   220→| ElevenLabs free minutes depleted | High | High | Limit live demos to 3. Pre-record a backup video of the demo |\n   221→| WebSpatial SDK bug | Medium | Medium | Stay on quick-example pattern. Don't customize beyond floating panels |\n   222→| Firebase not deploying | Low | High | Test deploy Feb 25. Keep localhost backup |\n   223→| Team exhaustion | Very High | High | Mandatory 2-hour sleep window 6-8AM. No exceptions |\n   224→\n   225→---\n   226→\n   227→## FALLBACK TIERS\n   228→\n   229→### Full Vision (all tech working):\n   230→WebSpatial Vision Pro + ElevenLabs clone voice + AMD embeddings + Firebase real-time\n   231→\n   232→### Tier 2 (no Mac Silicon):\n   233→WebSpatial in browser mode (still spatial-looking with CSS 3D) + ElevenLabs + AMD\n   234→\n   235→### Tier 3 (AMD not approved):\n   236→ElevenLabs clone + Firebase + React (still wins Best ElevenLabs + Best Freshman)\n   237→\n   238→### Minimum Viable Demo (everything else fails):\n   239→ElevenLabs conversational agent + photo upload + cloned voice responding to questions about photos. This STILL wins Best Freshman Hack and Best ElevenLabs.\n   240→\n   241→---\n   242→\n   243→## DEVPOST SUBMISSION CHECKLIST\n   244→\n   245→- [ ] Project name: MemoryBridge\n   246→- [ ] Tagline: \"Giving dementia patients their own voice back\"\n   247→- [ ] What it does (2 paragraphs)\n   248→- [ ] How we built it (list each tech + specific usage)\n   249→- [ ] Challenges we ran into\n   250→- [ ] Accomplishments proud of\n   251→- [ ] What we learned\n   252→- [ ] What's next (startup path)\n   253→- [ ] Screenshots: upload UI, voice clone before/after, WebSpatial room, memory conversation\n   254→- [ ] Demo video: 90-second screen recording\n   255→- [ ] Public GitHub repo link (repo created after 10AM 2/28)\n   256→- [ ] ALL team members added\n   257→- [ ] ALL team members have graduation year (2029) in Devpost profile\n   258→- [ ] Prizes opted into: Best Freshman Hack, Best ElevenLabs, Best AMD, Best WebSpatial, Future Unicorn, Responsible AI\n   259→\n   260→---\n   261→\n   262→## ETHICAL FRAMING (for judges who question it)\n   263→\n   264→The voice cloning preserves the patient's OWN voice from when they were healthy — not impersonating anyone else. The patient hears themselves remember — a form of self-continuity. This is analogous to home videos, but interactive. All consent is family-mediated. This is clinically aligned with reminiscence therapy (proven to slow cognitive decline). Frame it as digital preservation of personhood, not simulation.\n   265→\n   266→---\n   267→\n   268→## STARTUP PATH (for Ciocca Center prize pitch)\n   269→\n   270→**Business model:** $39/month family subscription OR $299/month per resident to nursing homes\n   271→**Market:** 55M dementia patients globally, 15,000 memory care facilities in the US alone\n   272→**Traction hook:** \"We built a pilot with 2 families this weekend\"\n   273→**Ask:** \"We're looking for mentors in healthtech and eldercare to help us take this to clinical trials\"\n   274→**Exit:** Acquisition target — Honor, Amedisys, Dementia UK, or Apple Healthcare\n   275→\n   276→---\n   277→\n   278→## RESEARCH INTELLIGENCE (from web research on sponsor preferences)\n   279→\n   280→### AMD — What Their Engineers Have Rewarded\n   281→- **1st place AMD Pervasive AI Contest 2026: \"Mather\"** — AI math tutor for students. Simple, educational, clear social impact. **MemoryBridge fits this exact profile.**\n   282→- AMD loves: on-device/privacy-first, social good, real-world utility, using ROCm stack correctly\n   283→- AMD currently running a **$160,000 Pervasive AI Developer Contest** — winning at H4H with AMD tech is a direct path to entering this bigger competition\n   284→- Frame the AMD angle as: \"semantic embeddings run privately on AMD GPU — no patient memory data leaves a secure environment\" (privacy + social good fusion)\n   285→\n   286→### ElevenLabs — What Wins Their Top Prize\n   287→- **Global winner GibberLink:** Won because it was novel at the **protocol level** — not just an app using TTS\n   288→- **MemoryBridge's protocol-level insight:** The cloned voice IS the data representation. Voice is the therapeutic intervention, not the UI layer. Frame it this way.\n   289→- Hugo Tour Guide won with location-aware, memory-connected voice conversations — MemoryBridge is this exact pattern but with personal life memories\n   290→- **ElevenLabs loves:** voice as primary modality (not bolted on), multi-language, real-time sub-200ms interaction, social good in high-stakes domains\n   291→\n   292→### WebSpatial — What ByteDance Wants to See\n   293→- 95% of developers at their Web-to-Spatial hackathon succeeded with the Quick Example — **do not over-engineer the spatial layer**\n   294→- ByteDance loves: native-quality feel from web tech, spatial data visualization with depth, health/educational tools in 3D\n   295→- The **memory room with photos floating by decade** is textbook \"spatial data visualization with emotional depth\"\n   296→\n   297→### The 2026 Meta-Trend: AGENTIC AI\n   298→- AMD (agentic edge apps), ElevenLabs (conversational agents), CrowdStrike (agentic SOC) — ALL sponsors are pushing toward autonomous multi-step AI\n   299→- MemoryBridge's conversational memory agent (autonomously retrieves, contextualizes, and narrates memories in response to questions) IS an agentic AI system\n   300→- In your Devpost and pitch: describe MemoryBridge as \"an agentic memory companion\" — this language will resonate with every sponsor judge\n   301→\n   302→### Eldercare is a 2026 Top Humanitarian AI Theme\n   303→- Harvard HSIL Hackathon 2026 is explicitly \"AI for Health Systems\" — same judging values\n   304→- SDG 3 (Good Health) was the #1 AI hackathon theme in 2025\n   305→- The APA reports AI/neuroscience converging on personalized mental health — dementia care is the intersection\n   306→\n   307→---\n   308→\n   309→## CRITICAL LINKS\n   310→\n   311→- AMD Developer Cloud: https://devcloud.amd.com\n   312→- WebSpatial Docs: https://webspatial.dev/docs/quick-example\n   313→- WebSpatial GitHub: https://github.com/webspatial/webspatial-sdk\n   314→- ElevenLabs Agents: https://elevenlabs.io/docs/agents-platform/quickstart\n   315→- ElevenLabs Voice Clone: https://elevenlabs.io/voice-cloning\n   316→- Firebase Console: https://console.firebase.google.com\n   317→- H4H Devpost: https://hack-for-humanity-2026.devpost.com/\n   318→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"acfb834a-55cc-40ae-88e0-b0b083731e44","timestamp":"2026-02-20T17:39:31.812Z","toolUseResult":{"type":"text","file":{"filePath":"C:\\Users\\ASUS\\Desktop\\h4h-2026-championship-plan.md","content":"# H4H 2026 Championship Plan — MemoryBridge\n\n## THE CHAMPION PROJECT: MemoryBridge\n\n> \"We give dementia patients their own voice back — to narrate their own memories.\"\n\n### Elevator Pitch (30 seconds)\n57 million people worldwide live with dementia. They forget their children's names, their wedding days, their own stories. MemoryBridge lets families upload photos and a 60-second voice recording of their loved one. Our AI clones their voice, builds a semantic memory graph from their life history, and creates an immersive spatial memory room on Apple Vision Pro — where they can walk through their past and hear themselves remember.\n\n---\n\n## PRIZE TARGETING (up to $9,000+)\n\n| Prize | Amount | How We Win It |\n|-------|--------|----------------|\n| Best Freshman Hack | $1,000 | ALL team members are freshmen; innovative + functional + well-designed |\n| Best Project Built with ElevenLabs | $1,980/member | Voice cloning IS the product, not a feature |\n| Best Use of AMD Tech | $1,000 | AMD GPU powers the semantic memory graph + embeddings |\n| Best Use of WebSpatial | $1,000 | Spatial memory room IS the core UX |\n| Future Unicorn Award | $1,000 | $350B eldercare market, clear B2B path |\n| Best Use of Responsible AI | $750 | Preserving dignity in aging, ethical framing |\n| Grand Prize (1st) | $2,250 | Every judge criteria: Tech ✓ Polish ✓ Innovativeness ✓ Social Impact ✓ |\n\n---\n\n## SYSTEM ARCHITECTURE\n\n```\n[Family] ──── React Upload Interface\n                    │\n                    ├── Firebase Storage ──── Photos + Voice Recording\n                    │\n                    └── Flask Backend (Python)\n                              │\n                    ┌─────────┴──────────────┐\n                    │                        │\n             AMD GPU Cloud              ElevenLabs\n         (sentence-transformers      (Instant Voice Clone\n          embeddings → semantic       + Conversational AI\n          memory graph)               Agent with knowledge base)\n                    │                        │\n                    └─────────┬──────────────┘\n                              │\n                    [Patient / Family]\n                              │\n                    WebSpatial (React)\n                    on Apple Vision Pro\n                    ────────────────────\n                    3D Spatial Memory Room:\n                    - Floating photo panels by era\n                    - Patient navigates by gaze + pinch\n                    - Reaches for photo → voice responds\n                    - Cloned voice narrates memories\n                    - Live conversational Q&A\n```\n\n---\n\n## TECH STACK\n\n| Layer | Technology | Why |\n|-------|-----------|-----|\n| Frontend | React + Vite + WebSpatial SDK | Workshop-taught, spatially renderable |\n| Backend | Flask + Python | Workshop-taught, fast to prototype |\n| Database | Firebase Firestore + Storage | Real-time, no-SQL, free tier generous |\n| AI Voice | ElevenLabs Instant Voice Clone + Conversational AI | 15-min hello world, voice IS the product |\n| AI Compute | AMD Developer Cloud (Instinct MI300X) | GPU for embeddings + LLM inference |\n| Spatial | WebSpatial SDK on visionOS (Apple Vision Pro) | ByteDance hardware provided at event |\n| Deployment | Firebase Hosting (frontend) + AMD Cloud (backend) | Both deployable in < 30 min |\n\n---\n\n## PRE-HACKATHON PREP (Feb 17-27) — CRITICAL\n\n### TODAY (Feb 17) — URGENT\n- [ ] **Apply for AMD Developer Cloud account** at devcloud.amd.com (3-BUSINESS-DAY approval!)\n- [ ] **Verify: Does any team member have a Mac with Apple Silicon (M1/M2/M3/M4)?** WebSpatial REQUIRES this\n- [ ] Create ElevenLabs account (free 1-month Creator tier at event, but test now)\n- [ ] Create Firebase project at console.firebase.google.com\n\n### Feb 18-19\n- [ ] Install Xcode + visionOS Simulator (large download — do this on good WiFi)\n- [ ] Run WebSpatial quick example: github.com/webspatial/quick-example\n- [ ] Test ElevenLabs voice clone with a 90-second clean voice recording\n- [ ] Set up Firebase Firestore + Storage rules (test mode)\n\n### Feb 20-22\n- [ ] Set up Flask + Firebase Admin SDK boilerplate\n- [ ] Install sentence-transformers: `pip install sentence-transformers`\n- [ ] Test AMD Developer Cloud account (if approved): run vLLM hello world\n- [ ] Build rough React photo upload component\n\n### Feb 23-25\n- [ ] Integrate WebSpatial into existing React app\n- [ ] Test: photo panel → spatial floating card on visionOS Simulator\n- [ ] Test: ElevenLabs agent with custom knowledge base (upload a test document)\n- [ ] Test: AMD GPU endpoint → embeddings → cosine similarity search\n\n### Feb 26-27 (Day Before)\n- [ ] Full dry run of the complete pipeline (upload → clone → spatial room → conversation)\n- [ ] Prepare DEMO AUDIO: clean 90-120 second voice recording (used during judging)\n- [ ] Prepare demo photo set (20-30 photos with dates/captions)\n- [ ] Team roles locked down\n- [ ] Create GitHub repo (note: must make it PUBLIC after 10AM on 2/28)\n- [ ] Draft Devpost description\n\n---\n\n## TEAM ROLES (2-4 people)\n\n### For 4 People:\n| Role | Person | Owns |\n|------|--------|------|\n| Frontend Lead | Person 1 | React UI, photo upload, memory timeline |\n| Spatial Lead | Person 2 | WebSpatial integration, Vision Pro UX |\n| Voice/AI Lead | Person 3 | ElevenLabs clone + Conversational agent |\n| Backend Lead | Person 4 | Flask, Firebase, AMD GPU embeddings |\n\n### For 2 People:\n| Role | Person | Owns |\n|------|--------|------|\n| Frontend + Spatial | Person 1 | React + WebSpatial + ElevenLabs widget |\n| Backend + AI | Person 2 | Flask + Firebase + AMD + ElevenLabs agent |\n\n---\n\n## HOUR-BY-HOUR BUILD PLAN (22 Hours)\n\n### Phase 1: Foundation (Hours 0-4, 11AM-3PM Feb 28)\n\n**11:00 AM — Hacking begins. Create GitHub repo immediately.**\n\n| Time | Person 1 (Frontend) | Person 2 (Backend) | Person 3 (Voice/AI) | Person 4 (Spatial) |\n|------|--------------------|--------------------|--------------------|--------------------|\n| 11-12PM | React + Vite setup | Flask + Firebase setup | ElevenLabs account + first voice clone | Install Xcode + WebSpatial |\n| 12-1PM | Photo upload UI | Firestore schema design | ElevenLabs Conversational agent | WebSpatial hello world |\n| 1-2PM | LUNCH BREAK | LUNCH BREAK | LUNCH BREAK | LUNCH BREAK |\n| 2-3PM | Photo grid UI | Flask → Firestore write | Agent knowledge base test | WebSpatial photo panel |\n\n**3PM: ATTEND AMD WORKSHOP — all hands. Take notes, get AMD account access.**\n\n### Phase 2: Core Features (Hours 4-10, 3PM-9PM Feb 28)\n\n| Time | What Gets Built |\n|------|----------------|\n| 3-4PM | AMD Workshop — everyone attends, note AMD GPU endpoint |\n| 4-5PM | AMD: Flask → AMD GPU → embeddings pipeline working |\n| 4-5PM | ElevenLabs: agent receives knowledge base (photo captions as text docs) |\n| 5-6PM | WebSpatial: multiple spatial panels with photos floating in 3D |\n| 5-6PM | Flask: POST /api/upload → Firebase Storage → trigger embedding job |\n| 6-7PM | DINNER BREAK |\n| 7-8PM | Integration test: upload photo → appears in WebSpatial room |\n| 7-8PM | ElevenLabs agent: can answer \"what happened in 1987?\" from knowledge base |\n| 8-9PM | Voice clone audio: prepare and run voice clone from demo audio |\n| 8-9PM | Present framing workshop attendance: \"How to Present\" at 8PM |\n\n### Phase 3: Integration Sprint (Hours 10-16, 9PM-3AM)\n\n| Time | What Gets Built |\n|------|----------------|\n| 9-10PM | Conversational agent now uses CLONED VOICE (swap voice in ElevenLabs dashboard) |\n| 10-11PM | WebSpatial: clicking/gazing at a photo triggers the voice agent to narrate that memory |\n| 11PM-12AM | Firebase real-time sync: new photos appear in spatial room without refresh |\n| 12-1AM | Polish: memory timeline sorted by era, visual design of spatial room |\n| 1-2AM | AMD semantic graph: visually show memory connections (bonus feature if time) |\n| 2-3AM | Integration testing end-to-end: upload → room → converse → clone voice responds |\n| 3AM | RAMEN BREAK — mandatory rest |\n\n### Phase 4: Polish + Demo Prep (Hours 16-22, 3AM-9AM Mar 1)\n\n| Time | What Gets Built |\n|------|----------------|\n| 3-5AM | Bug fixes, edge cases, loading states |\n| 5-6AM | Devpost submission started (add screenshots, description draft) |\n| 6-7AM | SLEEP (at least 1-2 hours mandatory — do not skip, demo quality tanks) |\n| 7-8AM | Final Devpost writeup + screenshots |\n| 8-8:30AM | Attend Devpost Q&A session |\n| 8:30-9AM | Submit Devpost. Final demo rehearsal. |\n\n**9:00 AM HARD DEADLINE: Devpost submitted.**\n\n---\n\n## DEMO SCRIPT (4 min + 1 min Q&A)\n\n### Opening (30s)\n> \"57 million people live with dementia today. By 2050, that number will triple. They forget their children's names. Their wedding days. The sound of their own voice when they were young. We built MemoryBridge to give that voice back.\"\n\n### Show the Web App (45s)\nDemonstrate the family upload interface — drag in 20 photos with dates/captions, upload a 90-second voice recording. Show the processing screen (\"Building your memory...\").\n\n### The Voice Clone Reveal (30s)\nPlay the original voice recording. Then play the ElevenLabs-cloned version. Let the room feel the uncanny similarity.\n> \"That voice will now live inside the memory room forever.\"\n\n### The Spatial Memory Room (60s)\nPut on the Apple Vision Pro (or show screencapture on main display). Navigate through floating photo panels organized by decade. Reach toward the wedding photo — it expands. The cloned voice speaks:\n> \"That was May 3rd, 1974. You wore your mother's dress. It rained in the morning but the sun came out just in time.\"\n\n### Live Conversational Demo (45s)\nAsk the agent out loud: \"Where did we go on our first vacation?\"\nThe system responds in the cloned voice with the answer drawn from the uploaded memories.\n> \"This is not a recording. This is a live AI conversation — in the patient's own voice.\"\n\n### The Impact (30s)\n> \"We tested this with two families during building. One caregiver said: 'My father hasn't recognized me in two years. But when I played this, he smiled.' That's why we built MemoryBridge. The eldercare market is $350 billion. We're starting with the most vulnerable people in it.\"\n\n### Technical Close (20s)\n> \"Built on ElevenLabs voice cloning and conversational AI, AMD GPU-accelerated semantic memory graphs, WebSpatial on Apple Vision Pro, and Firebase for real-time family collaboration.\"\n\n---\n\n## RISK MATRIX + MITIGATIONS\n\n| Risk | Probability | Impact | Mitigation |\n|------|-------------|--------|-----------|\n| AMD account not approved | Medium | High | Apply TODAY. Fallback: OpenAI API as LLM, note \"AMD in production\" |\n| No Mac with Apple Silicon | Medium | High | Check NOW. Fallback: WebSpatial web view without Vision Pro (still impressive) |\n| Voice clone sounds robotic | Medium | Critical | Use 90-120s clean audio. Pre-test Feb 26. Backup: use ElevenLabs' \"Aria\" voice |\n| ElevenLabs free minutes depleted | High | High | Limit live demos to 3. Pre-record a backup video of the demo |\n| WebSpatial SDK bug | Medium | Medium | Stay on quick-example pattern. Don't customize beyond floating panels |\n| Firebase not deploying | Low | High | Test deploy Feb 25. Keep localhost backup |\n| Team exhaustion | Very High | High | Mandatory 2-hour sleep window 6-8AM. No exceptions |\n\n---\n\n## FALLBACK TIERS\n\n### Full Vision (all tech working):\nWebSpatial Vision Pro + ElevenLabs clone voice + AMD embeddings + Firebase real-time\n\n### Tier 2 (no Mac Silicon):\nWebSpatial in browser mode (still spatial-looking with CSS 3D) + ElevenLabs + AMD\n\n### Tier 3 (AMD not approved):\nElevenLabs clone + Firebase + React (still wins Best ElevenLabs + Best Freshman)\n\n### Minimum Viable Demo (everything else fails):\nElevenLabs conversational agent + photo upload + cloned voice responding to questions about photos. This STILL wins Best Freshman Hack and Best ElevenLabs.\n\n---\n\n## DEVPOST SUBMISSION CHECKLIST\n\n- [ ] Project name: MemoryBridge\n- [ ] Tagline: \"Giving dementia patients their own voice back\"\n- [ ] What it does (2 paragraphs)\n- [ ] How we built it (list each tech + specific usage)\n- [ ] Challenges we ran into\n- [ ] Accomplishments proud of\n- [ ] What we learned\n- [ ] What's next (startup path)\n- [ ] Screenshots: upload UI, voice clone before/after, WebSpatial room, memory conversation\n- [ ] Demo video: 90-second screen recording\n- [ ] Public GitHub repo link (repo created after 10AM 2/28)\n- [ ] ALL team members added\n- [ ] ALL team members have graduation year (2029) in Devpost profile\n- [ ] Prizes opted into: Best Freshman Hack, Best ElevenLabs, Best AMD, Best WebSpatial, Future Unicorn, Responsible AI\n\n---\n\n## ETHICAL FRAMING (for judges who question it)\n\nThe voice cloning preserves the patient's OWN voice from when they were healthy — not impersonating anyone else. The patient hears themselves remember — a form of self-continuity. This is analogous to home videos, but interactive. All consent is family-mediated. This is clinically aligned with reminiscence therapy (proven to slow cognitive decline). Frame it as digital preservation of personhood, not simulation.\n\n---\n\n## STARTUP PATH (for Ciocca Center prize pitch)\n\n**Business model:** $39/month family subscription OR $299/month per resident to nursing homes\n**Market:** 55M dementia patients globally, 15,000 memory care facilities in the US alone\n**Traction hook:** \"We built a pilot with 2 families this weekend\"\n**Ask:** \"We're looking for mentors in healthtech and eldercare to help us take this to clinical trials\"\n**Exit:** Acquisition target — Honor, Amedisys, Dementia UK, or Apple Healthcare\n\n---\n\n## RESEARCH INTELLIGENCE (from web research on sponsor preferences)\n\n### AMD — What Their Engineers Have Rewarded\n- **1st place AMD Pervasive AI Contest 2026: \"Mather\"** — AI math tutor for students. Simple, educational, clear social impact. **MemoryBridge fits this exact profile.**\n- AMD loves: on-device/privacy-first, social good, real-world utility, using ROCm stack correctly\n- AMD currently running a **$160,000 Pervasive AI Developer Contest** — winning at H4H with AMD tech is a direct path to entering this bigger competition\n- Frame the AMD angle as: \"semantic embeddings run privately on AMD GPU — no patient memory data leaves a secure environment\" (privacy + social good fusion)\n\n### ElevenLabs — What Wins Their Top Prize\n- **Global winner GibberLink:** Won because it was novel at the **protocol level** — not just an app using TTS\n- **MemoryBridge's protocol-level insight:** The cloned voice IS the data representation. Voice is the therapeutic intervention, not the UI layer. Frame it this way.\n- Hugo Tour Guide won with location-aware, memory-connected voice conversations — MemoryBridge is this exact pattern but with personal life memories\n- **ElevenLabs loves:** voice as primary modality (not bolted on), multi-language, real-time sub-200ms interaction, social good in high-stakes domains\n\n### WebSpatial — What ByteDance Wants to See\n- 95% of developers at their Web-to-Spatial hackathon succeeded with the Quick Example — **do not over-engineer the spatial layer**\n- ByteDance loves: native-quality feel from web tech, spatial data visualization with depth, health/educational tools in 3D\n- The **memory room with photos floating by decade** is textbook \"spatial data visualization with emotional depth\"\n\n### The 2026 Meta-Trend: AGENTIC AI\n- AMD (agentic edge apps), ElevenLabs (conversational agents), CrowdStrike (agentic SOC) — ALL sponsors are pushing toward autonomous multi-step AI\n- MemoryBridge's conversational memory agent (autonomously retrieves, contextualizes, and narrates memories in response to questions) IS an agentic AI system\n- In your Devpost and pitch: describe MemoryBridge as \"an agentic memory companion\" — this language will resonate with every sponsor judge\n\n### Eldercare is a 2026 Top Humanitarian AI Theme\n- Harvard HSIL Hackathon 2026 is explicitly \"AI for Health Systems\" — same judging values\n- SDG 3 (Good Health) was the #1 AI hackathon theme in 2025\n- The APA reports AI/neuroscience converging on personalized mental health — dementia care is the intersection\n\n---\n\n## CRITICAL LINKS\n\n- AMD Developer Cloud: https://devcloud.amd.com\n- WebSpatial Docs: https://webspatial.dev/docs/quick-example\n- WebSpatial GitHub: https://github.com/webspatial/webspatial-sdk\n- ElevenLabs Agents: https://elevenlabs.io/docs/agents-platform/quickstart\n- ElevenLabs Voice Clone: https://elevenlabs.io/voice-cloning\n- Firebase Console: https://console.firebase.google.com\n- H4H Devpost: https://hack-for-humanity-2026.devpost.com/\n","numLines":318,"startLine":1,"totalLines":318}},"sourceToolAssistantUUID":"df4895a9-a453-47f1-819c-03627c1e1a3a"}
{"parentUuid":"acfb834a-55cc-40ae-88e0-b0b083731e44","isSidechain":false,"userType":"external","cwd":"C:\\Users\\ASUS","sessionId":"74318f24-6304-4f8c-88bf-94a57d613e9a","version":"2.1.47","gitBranch":"master","slug":"expressive-juggling-ladybug","message":{"model":"claude-sonnet-4-6","id":"msg_01QwQqnKQE21e7LG1gypwKr5","type":"message","role":"assistant","content":[{"type":"text","text":"Based on the plan, photos are uploaded via a **React web interface** (browser-based), not directly on the Apple Vision Pro.\n\nHere's the specific flow:\n\n1. **Family members use the React web upload UI** — they drag in 20-30 photos with dates/captions and upload a 90-second voice recording through a standard web app.\n\n2. **Photos go to Firebase Storage** via a Flask backend (`POST /api/upload → Firebase Storage`).\n\n3. **The Apple Vision Pro is the viewing/interaction device** — it displays the photos in the 3D spatial memory room, but is NOT where photos are uploaded.\n\nFrom the Demo Script (line 190):\n> *\"Demonstrate the family upload interface — drag in 20 photos with dates/captions, upload a 90-second voice recording. Show the processing screen ('Building your memory...').\"*\n\nSo the intended UX is:\n- **Family** → uploads photos on the **web app** (React in browser)\n- **Patient / Family** → experiences the spatial memory room on **Apple Vision Pro**\n\nThe two roles are separated by design — the upload is a web task for family members, while the Vision Pro is the therapeutic experience for the patient."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":5931,"cache_read_input_tokens":28120,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":5931},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CYKjJ8hNMFxNtejVUUSJi","type":"assistant","uuid":"9883f87f-7c9e-44e8-9cef-97a8a6b34702","timestamp":"2026-02-20T17:39:40.661Z"}
