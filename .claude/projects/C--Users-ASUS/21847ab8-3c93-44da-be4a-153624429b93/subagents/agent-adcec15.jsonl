{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"C:\\Users\\ASUS","sessionId":"21847ab8-3c93-44da-be4a-153624429b93","version":"2.1.41","gitBranch":"master","agentId":"adcec15","slug":"radiant-jingling-zephyr","type":"user","message":{"role":"user","content":"You are a **Senior Product Manager** at an early-stage startup.\n\nCONTEXT: PitchTok (TikTok-style deal sourcing for VCs/founders) is building a \"GrowthOps Agent\" — an AI-powered marketing automation system like OpenClaw but focused on founder sourcing and growth. The team is small (startup). Budget is limited. Speed matters.\n\nYour job: Create the ULTIMATE 8-week execution plan. This must be realistic, prioritized, and account for a small team.\n\nDeliver:\n\n1. **Product Vision & Strategy**:\n   - One-line vision\n   - Core value proposition\n   - MVP scope (what's in v1 vs what's deferred)\n   - Success metrics for v1\n\n2. **User Stories** (prioritized):\n   - As a growth operator, I want to...\n   - List at least 20 user stories with priority (P0/P1/P2)\n   - Acceptance criteria for each P0 story\n\n3. **8-Week Sprint Plan**:\n   Week 1-2: Foundation\n   Week 3-4: Core engine\n   Week 5-6: Automation + analytics\n   Week 7-8: Polish + launch\n   \n   For each week:\n   - Specific deliverables\n   - Dependencies\n   - Risks and mitigations\n   - Definition of done\n\n4. **Tech Decisions Matrix**:\n   For each technology choice, provide:\n   - Options considered\n   - Pros/cons\n   - Recommendation with justification\n   - Cost implications\n   \n   Decisions needed:\n   a) Database: Airtable vs Postgres vs Supabase\n   b) Backend: Node.js vs Python vs both\n   c) Frontend: Next.js vs Remix vs simple dashboard\n   d) Email: SendGrid vs Postmark vs SES\n   e) LLM: Claude vs GPT vs mix\n   f) Hosting: Vercel vs Railway vs AWS vs self-hosted\n   g) Queue: BullMQ vs Celery vs Inngest\n   h) CRM: Build vs Airtable vs HubSpot\n\n5. **Resource Plan**:\n   - What one person can build in 8 weeks\n   - What needs 2-3 people\n   - What to outsource\n   - What to use off-the-shelf\n\n6. **Risk Register**:\n   - Top 10 risks\n   - Probability and impact\n   - Mitigation strategies\n\n7. **Go-to-Market for the Agent Itself**:\n   - How to validate the agent works\n   - Metrics to track\n   - When to know if it's working\n   - Iteration strategy\n\nBe brutally realistic. No hand-waving. Actual deliverables, actual timelines, actual tradeoffs."},"uuid":"4c046eb3-c186-4e73-b9ab-c7b31b453d00","timestamp":"2026-02-15T00:55:58.953Z"}
{"parentUuid":"4c046eb3-c186-4e73-b9ab-c7b31b453d00","isSidechain":true,"userType":"external","cwd":"C:\\Users\\ASUS","sessionId":"21847ab8-3c93-44da-be4a-153624429b93","version":"2.1.41","gitBranch":"master","agentId":"adcec15","slug":"radiant-jingling-zephyr","message":{"model":"claude-opus-4-6","id":"msg_01SUkb2ufwnxVnw5PgNh5WAW","type":"message","role":"assistant","content":[{"type":"text","text":"\n\n# PitchTok GrowthOps Agent: 8-Week Execution Plan\n\n---\n\n## 1. Product Vision & Strategy\n\n### One-Line Vision\n\n**An autonomous AI agent that finds, qualifies, and warms up startup founders so VCs get a curated deal flow feed -- like a TikTok \"For You Page\" but for investments.**\n\n### Core Value Proposition\n\nVCs spend 60-70% of their sourcing time on manual grunt work: scanning Twitter/LinkedIn, reading newsletters, parsing Product Hunt launches, cold-emailing founders. The GrowthOps Agent eliminates this by continuously crawling founder signals, scoring them against a VC's thesis, generating personalized outreach, and delivering a ranked daily feed.\n\nFor founders seeking visibility: the agent surfaces their startup to relevant VCs without the cold-email lottery.\n\n### MVP Scope\n\n**In v1 (8 weeks):**\n- Founder signal ingestion from 3 sources: Twitter/X, Product Hunt, Crunchbase (free tier)\n- Thesis-matching scoring engine (LLM-based, configurable per VC)\n- Automated personalized email drafts (human-in-the-loop approval)\n- Daily digest feed (web dashboard)\n- Basic CRM: track outreach status (contacted / replied / meeting booked)\n- One VC firm as design partner (dogfooding)\n\n**Deferred to v2:**\n- LinkedIn scraping (legal complexity, API restrictions)\n- Autonomous sending (v1 is human-approved)\n- Video pitch integration (the \"TikTok\" piece -- ironic but correct to defer)\n- Multi-tenant SaaS (v1 is single-tenant)\n- Mobile app\n- Founder-side portal\n- Advanced analytics / attribution\n- Calendar integration for auto-scheduling\n\n### Success Metrics for v1\n\n| Metric | Target | Why It Matters |\n|--------|--------|----------------|\n| Founders sourced per day | 20+ qualified leads | Proves the engine works |\n| Thesis match accuracy | >70% (VC confirms relevance) | Core value prop |\n| Email draft acceptance rate | >50% sent without major edits | Measures LLM quality |\n| Reply rate on outreach | >15% (cold email benchmark: 5-8%) | Proves personalization works |\n| Meetings booked (Week 7-8) | 5+ in two weeks | End-to-end validation |\n| Time saved vs manual sourcing | >10 hours/week per VC partner | ROI justification |\n\n---\n\n## 2. User Stories (Prioritized)\n\n### P0 -- Must Have for MVP\n\n| # | User Story | Acceptance Criteria |\n|---|-----------|-------------------|\n| 1 | As a growth operator, I want to define a VC's investment thesis in natural language so the agent knows what to look for | Thesis saved as structured config. Agent can parse \"Series A B2B SaaS in fintech, US/EU, $1-5M ARR\" into filterable criteria. Validated by running against 50 known companies and confirming >70% correct classification. |\n| 2 | As a growth operator, I want the agent to ingest founder signals from Twitter/X daily so I have fresh leads | Cron job runs daily. Captures tweets matching keyword/account lists. Deduplicates. Stores structured data (name, handle, company, tweet content, follower count). Process completes in <30 min for 500 accounts. |\n| 3 | As a growth operator, I want the agent to ingest new Product Hunt launches so I catch emerging startups | Daily scrape of PH \"new\" and \"trending.\" Extracts founder name, product description, upvote count, links. Stored in unified founder table. |\n| 4 | As a growth operator, I want each founder scored against the thesis so I can focus on the best matches | Score 0-100 with explanation. Uses LLM with thesis + founder signals as context. Scores cached. Re-scoring on new signals. Scoring pipeline handles 200+ founders/day within API budget. |\n| 5 | As a growth operator, I want a daily digest dashboard showing top-scored founders | Web page, sorted by score. Shows founder name, company, score, top signal, one-line summary. Filterable by score threshold. Loads in <2s. |\n| 6 | As a growth operator, I want AI-generated personalized outreach emails for each founder | Draft references specific founder signals (their tweet, their PH launch). Tone matches VC's voice (configurable template). Draft stored, editable before sending. |\n| 7 | As a growth operator, I want to approve/edit/reject email drafts before they send | Queue of pending drafts. One-click approve, inline edit, or reject. Approved emails sent via configured email service. Rejection feeds back to improve future drafts. |\n| 8 | As a growth operator, I want to track outreach status per founder | States: New, Contacted, Replied, Meeting Booked, Passed. Manual state transitions. Timestamp on each transition. |\n| 9 | As a growth operator, I want the agent to deduplicate founders across sources | Matching on name + company (fuzzy). Merge signals into single founder profile. No duplicate outreach. |\n| 10 | As a growth operator, I want to configure which Twitter accounts and keywords to monitor | Admin UI to add/remove accounts and keywords. Changes take effect on next daily run. |\n\n### P1 -- Important but Can Ship Without\n\n| # | User Story |\n|---|-----------|\n| 11 | As a growth operator, I want Crunchbase enrichment for each founder (funding history, company details) |\n| 12 | As a growth operator, I want to see why the agent scored a founder high (explainability) |\n| 13 | As a growth operator, I want weekly performance reports (leads sourced, emails sent, replies received) |\n| 14 | As a growth operator, I want to snooze a founder and revisit them later |\n| 15 | As a growth operator, I want email open/click tracking to gauge interest |\n| 16 | As a growth operator, I want to tag founders with custom labels (e.g., \"hot lead\", \"portfolio fit\") |\n| 17 | As a growth operator, I want the agent to detect when a founder I'm tracking tweets something new |\n\n### P2 -- Nice to Have / Post-MVP\n\n| # | User Story |\n|---|-----------|\n| 18 | As a growth operator, I want the agent to learn from my approve/reject patterns to improve scoring |\n| 19 | As a growth operator, I want Slack notifications for high-score founders (>85) |\n| 20 | As a growth operator, I want to import a CSV of existing contacts to avoid re-outreach |\n| 21 | As a growth operator, I want multi-sequence drip campaigns (follow-up emails) |\n| 22 | As a growth operator, I want to compare sourcing performance across different thesis configurations |\n| 23 | As a growth operator, I want the agent to monitor HackerNews \"Show HN\" posts |\n| 24 | As a growth operator, I want calendar integration to auto-book meetings when a founder replies with availability |\n\n---\n\n## 3. Eight-Week Sprint Plan\n\n### Week 1-2: Foundation\n\n**Goal:** Data pipeline running, founders appearing in a database, basic dashboard skeleton.\n\n**Week 1 Deliverables:**\n- Project scaffolding: monorepo, CI/CD, dev environment\n- Database schema: founders, signals, thesis_configs, outreach tables\n- Twitter/X ingestion pipeline (using Twitter API v2 free/basic tier)\n- Product Hunt scraper (unofficial API or RSS)\n- Raw data landing in database, deduplication logic\n\n**Week 2 Deliverables:**\n- Thesis configuration data model + seed one real VC thesis\n- Basic scoring pipeline: LLM call per founder with thesis context\n- Dashboard skeleton: Next.js app with auth, founders list page\n- Daily cron job orchestration (Inngest or simple cron)\n\n**Dependencies:**\n- Twitter API access approved (apply Day 1)\n- Design partner VC commits thesis input by end of Week 1\n- LLM API key provisioned\n\n**Risks & Mitigations:**\n| Risk | Probability | Mitigation |\n|------|-------------|-----------|\n| Twitter API rate limits on free tier | High | Start with Basic tier ($100/mo). Fallback: use Apify Twitter scraper ($49/mo). |\n| Product Hunt blocks scraping | Medium | Use RSS feed as primary. PH API waitlist as backup. |\n| LLM scoring too slow for 200+ founders | Medium | Batch calls. Use Claude Haiku for initial scoring, Sonnet for top-30 re-ranking. |\n\n**Definition of Done:**\n- 50+ founders ingested from real sources into database\n- At least one founder scored against a real thesis\n- Dashboard loads and shows a list of founders with scores\n- All code in Git, CI passing\n\n---\n\n### Week 3-4: Core Engine\n\n**Goal:** Scoring is accurate, outreach drafts are generated, the daily workflow is usable end-to-end.\n\n**Week 3 Deliverables:**\n- Scoring prompt engineering: iterate with design partner on 50 founders, hit >70% accuracy\n- Founder profile enrichment (Crunchbase free data where available)\n- Founder detail page on dashboard (all signals, score explanation)\n- Email draft generation pipeline (LLM generates personalized email per founder)\n\n**Week 4 Deliverables:**\n- Outreach approval queue UI (approve / edit / reject)\n- Email sending integration (Postmark transactional)\n- Outreach status tracking (CRM-lite states)\n- Deduplication refinement (fuzzy matching tuning)\n- Thesis configuration UI (admin can edit thesis in dashboard)\n\n**Dependencies:**\n- Scoring accuracy validated by design partner (Week 3 checkpoint)\n- Postmark account + sending domain DNS configured\n- Design partner provides 2-3 sample outreach emails for voice calibration\n\n**Risks & Mitigations:**\n| Risk | Probability | Mitigation |\n|------|-------------|-----------|\n| Scoring accuracy below 70% | Medium | Add more context signals. Let VC provide positive/negative examples for few-shot prompting. |\n| Email drafts sound generic | High | Include 2-3 specific founder signals in prompt. A/B test prompt variants. Have VC grade 20 drafts. |\n| Founder email addresses unavailable | High | Use Hunter.io or Apollo.io API for email finding. Budget $50-100/mo. Fallback: Twitter DM as channel. |\n\n**Definition of Done:**\n- Design partner confirms >70% scoring accuracy on a batch of 100 founders\n- 10 outreach emails drafted, reviewed, and approved through the UI\n- At least 5 emails actually sent to real founders\n- CRM states updating correctly\n\n---\n\n### Week 5-6: Automation & Analytics\n\n**Goal:** System runs daily with minimal intervention. Basic analytics prove value.\n\n**Week 5 Deliverables:**\n- Fully automated daily pipeline: ingest, score, generate drafts, notify operator\n- Email open/click tracking (Postmark webhooks)\n- Reply detection (inbound email parsing or manual logging)\n- Weekly summary report (founders sourced, emails sent, replies, meetings)\n\n**Week 6 Deliverables:**\n- Signal freshness: re-score founders when new signals arrive\n- Performance tuning: scoring pipeline handles 200+ founders/day\n- Dashboard polish: filters, search, bulk actions\n- Monitoring/alerting: pipeline failure notifications (Slack or email)\n\n**Dependencies:**\n- Enough outreach data (Week 3-4) to have meaningful analytics\n- Postmark webhook endpoint deployed and tested\n\n**Risks & Mitigations:**\n| Risk | Probability | Mitigation |\n|------|-------------|-----------|\n| Low reply rates make analytics meaningless | Medium | Supplement with open/click rates. Iterate on email copy. 15% reply rate is ambitious for cold outreach -- 8-10% is still valuable. |\n| Pipeline breaks silently | High | Add health checks. Slack alert on any pipeline step failure. Dead-letter queue for failed items. |\n| Scoring costs escalate with volume | Medium | Cache scores. Only re-score on new signals. Use Haiku for bulk, Sonnet for top candidates. Budget cap: $200/mo LLM spend. |\n\n**Definition of Done:**\n- Pipeline runs 5 consecutive days without manual intervention\n- Weekly report generated automatically\n- Open/reply tracking functional on sent emails\n- Dashboard loads <2s with 500+ founders in database\n\n---\n\n### Week 7-8: Polish & Launch\n\n**Goal:** Production-ready for design partner. Metrics prove the agent works. Decision point on scaling.\n\n**Week 7 Deliverables:**\n- Bug fixes from design partner feedback\n- Onboarding flow: new thesis setup wizard\n- Data export (CSV of founders, outreach history)\n- Documentation: operator playbook, system architecture\n\n**Week 8 Deliverables:**\n- Performance review with design partner (all success metrics evaluated)\n- Security audit (API keys, data handling, email compliance -- CAN-SPAM headers)\n- Load testing: can handle 2-3 VC firms simultaneously (prep for multi-tenant)\n- Launch decision: continue, pivot, or kill based on metrics\n- Demo video / deck for next customers\n\n**Dependencies:**\n- Design partner available for Week 7 feedback sessions\n- Minimum 2 weeks of outreach data for meaningful metrics review\n\n**Risks & Mitigations:**\n| Risk | Probability | Mitigation |\n|------|-------------|-----------|\n| Metrics don't hit targets | Medium | Have \"good enough\" thresholds defined in advance. 60% accuracy + 10% reply rate is still viable. Identify which metric failed and why. |\n| Design partner disengages | Low | Weekly check-ins. Show value early (Week 3 demo). Have a backup VC contact. |\n| CAN-SPAM / GDPR compliance gaps | Medium | Add unsubscribe link to all emails. Don't store EU personal data without consent notice. Legal review of outreach copy. |\n\n**Definition of Done:**\n- All P0 stories complete and tested\n- Design partner using the system daily\n- Success metrics measured and documented\n- Go/no-go decision made with data\n\n---\n\n## 4. Tech Decisions Matrix\n\n### a) Database\n\n| Option | Pros | Cons | Cost |\n|--------|------|------|------|\n| **Airtable** | Fast to start, UI built-in, non-technical team can edit | 1,000 record limit on free tier, slow API, no real joins, vendor lock-in | $20/mo (Plus) |\n| **Postgres (via Supabase)** | Real database, SQL, scales, free tier generous (500MB), built-in auth + REST API | More setup than Airtable, need to build UI | Free tier, then $25/mo |\n| **Raw Postgres (self-hosted)** | Full control | Ops burden, no auth layer | $5-15/mo (Railway/Render) |\n\n**Recommendation: Supabase (Postgres).** You get a real database with a generous free tier, built-in auth for the dashboard, REST API for quick prototyping, and you avoid Airtable's record limits which you will hit within Week 2 at 200+ founders/day. Migration cost from Airtable later would be painful.\n\n### b) Backend\n\n| Option | Pros | Cons | Cost |\n|--------|------|------|------|\n| **Python (FastAPI)** | Best LLM ecosystem (langchain, litellm), great for data pipelines, team likely knows it | Async is less ergonomic than Node for webhooks | Free |\n| **Node.js (Express/Hono)** | Great for webhooks/real-time, large ecosystem | LLM libraries less mature, data processing less natural | Free |\n| **Both** | Best of both worlds | Complexity, two runtimes to maintain | Free but costly in time |\n\n**Recommendation: Python (FastAPI).** The core value is in the data/LLM pipeline, not in real-time web features. Python's ecosystem for LLM orchestration, data processing, and scraping is unmatched. FastAPI gives you async where needed and auto-generated API docs. One language, one runtime, less cognitive overhead for a small team.\n\n### c) Frontend\n\n| Option | Pros | Cons | Cost |\n|--------|------|------|------|\n| **Next.js** | Full-stack, SSR, huge ecosystem, Vercel deployment | Heavier than needed for an internal dashboard | Free |\n| **Remix** | Better data loading patterns | Smaller ecosystem, less hiring pool | Free |\n| **Simple dashboard (Streamlit / Retool)** | Fastest to build, no frontend code | Limited customization, looks like a prototype | Retool: $10/mo, Streamlit: free |\n\n**Recommendation: Next.js for the dashboard.** Streamlit is tempting for speed but becomes a liability when you need approval queues, inline editing, and real-time updates. Next.js with a component library (shadcn/ui) gets you a production-quality dashboard in the same time as a polished Streamlit app, with none of the ceiling. If the team has zero frontend experience, use Retool for Weeks 1-4 and migrate to Next.js for Week 5+.\n\n### d) Email\n\n| Option | Pros | Cons | Cost |\n|--------|------|------|------|\n| **SendGrid** | Well-known, good free tier (100/day) | Deliverability reputation has declined, complex UI | Free to $20/mo |\n| **Postmark** | Best deliverability, excellent webhook support, simple API | No free tier (but cheap) | $15/mo (10K emails) |\n| **AWS SES** | Cheapest at scale ($0.10/1K emails) | More setup, less analytics, deliverability requires warm-up | ~$1-5/mo |\n\n**Recommendation: Postmark.** For cold outreach to founders, deliverability is everything. Postmark has the best inbox placement rates. The cost difference ($15/mo vs free) is irrelevant at this stage. Their webhook support for open/click/reply tracking is also the cleanest to implement.\n\n### e) LLM\n\n| Option | Pros | Cons | Cost |\n|--------|------|------|------|\n| **Claude (Anthropic)** | Best at following complex instructions, good at nuanced writing, large context window | Slightly slower for simple tasks | Haiku: ~$0.25/1M tokens, Sonnet: ~$3/1M |\n| **GPT-4o / GPT-4o-mini** | Fast, good function calling, widely used | Writing can be formulaic, less nuanced scoring | Mini: ~$0.15/1M, 4o: ~$2.50/1M |\n| **Mix** | Best model per task | More complexity, two API integrations | Varies |\n\n**Recommendation: Claude (tiered).** Use Haiku for bulk scoring (cheap, fast, 90% as good). Use Sonnet for email drafting and re-ranking top candidates (better writing quality). Single provider simplifies billing and API management. Estimated cost: $50-150/mo at 200 founders/day.\n\n### f) Hosting\n\n| Option | Pros | Cons | Cost |\n|--------|------|------|------|\n| **Vercel** | Best Next.js deployment, edge functions | Not ideal for long-running Python jobs | Free to $20/mo |\n| **Railway** | Simple, supports Python + Node, good for background jobs | Less mature, occasional reliability issues | $5-20/mo |\n| **AWS (ECS/Lambda)** | Scales infinitely, cheapest at scale | Complex setup, overkill for MVP | $10-50/mo |\n| **Render** | Simple, good free tier, supports cron jobs | Cold starts on free tier | Free to $25/mo |\n\n**Recommendation: Vercel (frontend) + Railway (Python backend + cron jobs).** Split deployment: Next.js dashboard on Vercel (free tier, optimized), Python pipeline on Railway ($5/mo, supports cron, easy deployment). Total: $5-25/mo. Migrate to AWS only if you need to scale past 5 VC firms.\n\n### g) Queue / Job Orchestration\n\n| Option | Pros | Cons | Cost |\n|--------|------|------|------|\n| **BullMQ** | Mature, Redis-backed, good monitoring | Node.js only | Free + Redis ($0-10/mo) |\n| **Celery** | Python-native, battle-tested | Complex setup (Redis/RabbitMQ), heavy for a small project | Free + broker cost |\n| **Inngest** | Event-driven, built-in retries/scheduling, great DX, works with any runtime | Newer, potential vendor lock-in | Free tier (generous) |\n| **Simple cron + DB queue** | No dependencies, easy to understand | No retries, no monitoring, fragile | Free |\n\n**Recommendation: Inngest.** For a small team, Inngest's developer experience is unmatched. Event-driven functions with built-in retries, scheduling, and observability. Free tier handles MVP volume easily. You avoid standing up Redis/RabbitMQ infrastructure. If Inngest doesn't support your Python runtime well, fall back to a simple DB-backed queue with APScheduler for cron.\n\n### h) CRM\n\n| Option | Pros | Cons | Cost |\n|--------|------|------|------|\n| **Build custom (in Supabase)** | Exactly what you need, no integration overhead | Dev time | Free (part of Supabase) |\n| **Airtable as CRM** | Visual, easy to share with VC partner, fast to set up | Record limits, API rate limits, sync complexity | $20/mo |\n| **HubSpot** | Full-featured, free CRM tier | Massive overkill, complex API, opinionated workflows | Free to $50/mo |\n\n**Recommendation: Build custom in Supabase.** Your CRM needs are simple: founder record + outreach status + timestamps. That is 2-3 database tables, not a SaaS product. Building it in Supabase keeps everything in one place, avoids sync issues, and the dashboard already reads from this database. If the VC partner demands HubSpot, build a one-way sync later.\n\n### Cost Summary (Monthly)\n\n| Item | Cost |\n|------|------|\n| Supabase (database + auth) | $0-25 |\n| Vercel (frontend) | $0-20 |\n| Railway (backend) | $5-20 |\n| Postmark (email) | $15 |\n| Claude API (LLM) | $50-150 |\n| Twitter API (Basic) | $100 |\n| Hunter.io (email finding) | $49 |\n| Inngest (job orchestration) | $0 |\n| Domain + DNS | $12/yr |\n| **Total** | **$220-380/mo** |\n\n---\n\n## 5. Resource Plan\n\n### What One Person Can Build in 8 Weeks\n\nOne strong full-stack developer (Python + basic React/Next.js) can deliver all P0 stories if they:\n- Use Supabase (saves building auth, API layer)\n- Use a component library like shadcn/ui (saves design time)\n- Skip pixel-perfect UI (functional > pretty)\n- Use Inngest or simple cron (no infrastructure management)\n- Limit to 3 data sources (Twitter, Product Hunt, Crunchbase enrichment)\n\nThis is tight but doable. The critical path is: data pipeline (Week 1-2) then LLM scoring/outreach (Week 3-4) then dashboard (built incrementally). One person will cut corners on testing and monitoring.\n\n### What Needs 2-3 People\n\nWith 2 developers:\n- Dev 1 (Backend/ML): Data pipelines, LLM scoring, email integration, cron jobs\n- Dev 2 (Full-stack): Dashboard, approval queue, CRM UI, analytics\n\nThis is the ideal team size. Parallel work on backend and frontend. Ship P0 + most P1 stories in 8 weeks.\n\nWith a 3rd person (part-time):\n- Person 3 (Growth/Ops): Design partner management, outreach copy iteration, metric tracking, manual QA of scoring accuracy\n\n### What to Outsource\n\n| Task | Outsource To | Cost |\n|------|-------------|------|\n| Logo + basic brand | Fiverr / existing designer | $50-200 |\n| Legal review of outreach compliance | Startup lawyer (1 hour) | $300-500 |\n| Email deliverability setup (SPF/DKIM/DMARC) | Postmark docs (self-serve) | $0 |\n| Penetration testing | Defer to post-MVP | $0 |\n\n### What to Use Off-the-Shelf\n\n| Need | Solution | Why Not Build |\n|------|----------|--------------|\n| Auth | Supabase Auth | Never build auth |\n| Email sending | Postmark | Deliverability is hard |\n| Email finding | Hunter.io API | Data acquisition is not your moat |\n| Twitter data | Twitter API / Apify | Scraping is fragile |\n| Monitoring | Sentry (free tier) | Observability is table stakes |\n| Analytics | PostHog (free tier) | Event tracking solved problem |\n\n---\n\n## 6. Risk Register\n\n| # | Risk | Probability | Impact | Mitigation |\n|---|------|-------------|--------|-----------|\n| 1 | **Twitter API access revoked or repriced** | Medium | Critical | Maintain Apify scraper as fallback. Diversify sources so no single source is >40% of leads. |\n| 2 | **LLM scoring accuracy below 60%** | Medium | High | Few-shot examples from design partner. Ensemble scoring (multiple prompts, average). Manual override. Weekly prompt tuning sessions. |\n| 3 | **Cold emails land in spam** | High | High | Use Postmark (best deliverability). Warm sending domain for 2 weeks before launch. Keep volume <50/day initially. Personalization helps deliverability. |\n| 4 | **Design partner disengages mid-project** | Low | Critical | Weekly demos with clear value. Have 2 backup VC contacts. Build something useful even without a partner (the agent can source for your own startup). |\n| 5 | **Founder email addresses not findable** | Medium | High | Hunter.io + Apollo.io as backup. Fallback to Twitter DM. Accept that 30-40% of founders won't have findable emails. |\n| 6 | **Scope creep from design partner requests** | High | Medium | Strict P0/P1/P2 prioritization. Say no to anything not on the list until Week 8. Weekly scope review. |\n| 7 | **Solo developer burnout (if 1-person team)** | Medium | High | Realistic sprint targets. No weekend work. Use off-the-shelf tools aggressively. Cut P1 scope if behind. |\n| 8 | **CAN-SPAM / GDPR legal exposure** | Low | Critical | Unsubscribe link on every email. Don't email EU founders without consent mechanism. Consult lawyer for 1 hour ($300). |\n| 9 | **Data quality: duplicate/stale founder profiles** | High | Medium | Fuzzy matching (name + company). Freshness timestamp. Auto-archive founders with no new signals in 30 days. |\n| 10 | **LLM costs exceed budget at scale** | Low | Medium | Haiku for bulk scoring ($0.25/1M tokens). Cache scores. Set hard budget cap with API provider alerts. At 200 founders/day, cost is ~$3-5/day. |\n\n---\n\n## 7. Go-to-Market for the Agent Itself\n\n### Validation Framework\n\n**Phase 1: Shadow Mode (Week 3-4)**\n- Run the agent alongside the design partner's existing manual process\n- Compare: Did the agent find the same founders they found manually? Did it find better ones they missed?\n- Target: Agent finds 80% of what the human found + 20% net-new relevant founders\n\n**Phase 2: Assisted Mode (Week 5-6)**\n- Design partner uses the agent as their primary sourcing tool\n- Human reviews and approves all outreach\n- Track: time spent per day on sourcing (should drop from 2-3 hours to 30 minutes)\n\n**Phase 3: Semi-Autonomous Mode (Week 7-8)**\n- Agent runs daily, operator reviews digest once per day\n- Outreach sent with one-click approval\n- Track end-to-end metrics (founders sourced to meetings booked)\n\n### Metrics to Track\n\n**Leading Indicators (Week 1-4):**\n- Founders ingested per day (target: 50+)\n- Scoring accuracy (target: >70% confirmed relevant by VC)\n- Email draft quality (target: >50% approved without major edits)\n\n**Lagging Indicators (Week 5-8):**\n- Emails sent per week (target: 30-50)\n- Reply rate (target: >15%)\n- Meetings booked per week (target: 2-3)\n- Time saved per week (target: >10 hours)\n- Net Promoter Score from design partner (target: >8/10)\n\n### When to Know It Is Working\n\n**Green light (continue and scale):**\n- Reply rate >12% AND meetings booked >1/week AND design partner says \"I can't go back to manual\"\n\n**Yellow light (iterate):**\n- Reply rate 5-12% OR scoring accuracy 50-70% -- the system adds value but needs tuning. Invest 2 more weeks in prompt engineering and source quality.\n\n**Red light (pivot or kill):**\n- Reply rate <5% AND scoring accuracy <50% AND design partner stops using it within 2 weeks. The problem may not be solvable with current data sources, or the value prop is wrong.\n\n### Iteration Strategy\n\n**Weekly cadence:**\n1. Monday: Review last week's metrics\n2. Tuesday: Prompt tuning based on false positives/negatives from design partner feedback\n3. Wednesday-Thursday: Ship improvements\n4. Friday: Design partner check-in, collect qualitative feedback\n\n**Prompt versioning:** Every scoring/drafting prompt gets a version number. A/B test new prompts against the current best on the same batch of founders. Only promote if measurably better.\n\n**Source expansion decision:** Only add a new data source (HackerNews, LinkedIn, etc.) when existing sources are fully optimized. Adding sources is easy. Making them high-quality is hard.\n\n---\n\n## Summary: The Honest Truth\n\nThis is an ambitious but achievable plan for a 2-person team over 8 weeks with a budget of $300-400/month in infrastructure. The critical success factor is not the technology -- it is the design partner relationship and the quality of LLM prompts for scoring and outreach.\n\nThe biggest trap is building too much infrastructure and not enough intelligence. The agent's value is in three things:\n1. Finding founders the VC would not have found manually\n2. Scoring them accurately against a real thesis\n3. Writing outreach that gets replies\n\nEverything else -- the dashboard, the CRM, the analytics -- is scaffolding. Spend 60% of engineering time on those three capabilities and 40% on everything else.\n\nIf Week 4 arrives and scoring accuracy is below 60%, stop building features and spend Week 5-6 exclusively on prompt engineering and data quality. A beautiful dashboard showing bad recommendations is worthless."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":3170,"cache_read_input_tokens":11845,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":3170},"output_tokens":3,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY8wixfvALZ38Cr341E65","type":"assistant","uuid":"75422fc4-e303-45b9-b1ac-0b3033ac9531","timestamp":"2026-02-15T00:58:58.698Z"}
{"parentUuid":"75422fc4-e303-45b9-b1ac-0b3033ac9531","isSidechain":true,"userType":"external","cwd":"C:\\Users\\ASUS","sessionId":"21847ab8-3c93-44da-be4a-153624429b93","version":"2.1.41","gitBranch":"master","agentId":"adcec15","slug":"radiant-jingling-zephyr","type":"user","message":{"role":"user","content":"Please output your complete findings as a single comprehensive response. Output everything you researched and designed."},"uuid":"8ea77266-9f23-4c3c-9ae0-85fcc2499716","timestamp":"2026-02-15T01:02:02.083Z"}
{"parentUuid":"8ea77266-9f23-4c3c-9ae0-85fcc2499716","isSidechain":true,"userType":"external","cwd":"C:\\Users\\ASUS","sessionId":"21847ab8-3c93-44da-be4a-153624429b93","version":"2.1.41","gitBranch":"master","agentId":"adcec15","slug":"radiant-jingling-zephyr","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01G2bDxL9PVwaTs9VW1zAnoj","type":"message","role":"assistant","content":[{"type":"text","text":"# PitchTok GrowthOps Agent: Complete 8-Week Execution Plan\n\n**A realistic, prioritized, actionable roadmap for building an AI-powered founder sourcing agent for VCs.**\n\n---\n\n## EXECUTIVE SUMMARY\n\nPitchTok's GrowthOps Agent solves a $2B+ opportunity: VCs spend 60-70% of sourcing time on grunt work (Twitter scanning, newsletter parsing, cold email drafting). The agent automates this into a daily curated feed of qualified founders with AI-generated, personalized outreach—reducing sourcing time from 2-3 hours/day to 30 minutes while improving reply rates from 5-8% (baseline cold email) to 15%+.\n\n**Core constraints:**\n- Small team (1-2 engineers)\n- 8-week timeline to MVP\n- $300-400/month total infrastructure budget\n- Must validate with real VC design partner\n\n**The plan trades off comprehensiveness for speed:** we're building the Minimum Viable Agent, not the Ultimate System. We defer video integration, founder-side portal, true autonomous sending, and multi-tenant SaaS until we prove the core value prop with one VC firm.\n\n---\n\n## 1. PRODUCT VISION & STRATEGY\n\n### One-Line Vision\n\n**An autonomous AI agent that finds, qualifies, and warms up startup founders so VCs get a curated deal flow feed—like TikTok's \"For You Page\" but for investments.**\n\n### Core Value Proposition\n\n**For VCs/Growth Operators:**\n- **Problem:** Manual sourcing is 60% busy work (scanning Twitter, reading newsletters, parsing Product Hunt, cold-emailing founders). High-quality deal flow is still hit-or-miss.\n- **Solution:** The agent continuously crawls founder signals (Twitter tweets, Product Hunt launches, news), scores them against the VC's thesis in real-time, and generates personalized outreach. Operator gets a ranked daily digest of 20-50 qualified founders.\n- **Outcome:** 10+ hours/week saved + higher reply rates (15% vs 5-8% baseline) + better deal flow quality (thesis-aligned founders only).\n\n**For founders (post-MVP):**\n- Visibility to VCs without the cold-email lottery or expensive PR firms\n\n### MVP Scope\n\n#### What's In (v1, 8 weeks)\n\n| Layer | Component |\n|-------|-----------|\n| **Data Ingestion** | Twitter/X (keyword + account tracking), Product Hunt (daily launches), Crunchbase (free tier enrichment) |\n| **Intelligence** | LLM-based thesis scoring (Claude), signal aggregation, deduplication |\n| **Outreach** | AI-generated email drafts with human-in-the-loop approval, Postmark integration |\n| **CRM-lite** | Founder profiles, outreach status tracking (New → Contacted → Replied → Meeting), signal history |\n| **Dashboard** | Daily digest feed, founder detail view, approval queue, basic analytics |\n| **Automation** | Daily cron job, email open/click tracking, reply detection |\n\n#### What's Deferred (v2+)\n\n| Feature | Why Deferred | Timeline |\n|---------|-------------|----------|\n| **LinkedIn scraping** | Legal complexity, API restrictions, rate limits | v1.5 (Week 12+) |\n| **Autonomous sending** | Requires more legal/brand safety review with VC | v1.5 |\n| **Video pitch (the \"TikTok\" piece)** | Storage, transcription, multi-modal scoring -- scope explosion | v2 (3+ months) |\n| **Multi-tenant SaaS** | Authentication, data isolation, billing -- adds 4+ weeks | v2 |\n| **Mobile app** | Dashboard-first for MVP, mobile later | v2 |\n| **Founder-side portal** | Requires founder authentication, more features | v2 |\n| **Advanced analytics** | Attribution, A/B testing, cohort analysis | v2 |\n| **Calendar integration** | Nice-to-have, can be manual for now | v2 |\n\n### Success Metrics for v1\n\n**Why These Metrics?** They directly measure whether the agent solves the core problem: finding better founders faster.\n\n| Metric | Target | Measurement | Why It Matters |\n|--------|--------|-------------|----------------|\n| **Founders sourced per day** | 50+ | Count in database | Proves pipeline has volume |\n| **Thesis match accuracy** | >70% | VC confirms relevance | Core value prop: \"I actually want to talk to these founders\" |\n| **Email draft acceptance rate** | >50% sent without major edits | Count approved ÷ drafted | Measures LLM quality; drafts must be usable, not just technically correct |\n| **Reply rate on outreach** | >15% | Replies ÷ sent emails | Cold email baseline: 5-8%. 15%+ proves personalization + signal quality work. |\n| **Meetings booked (Weeks 7-8)** | 5+ in two weeks | Calendar confirms | End-to-end proof: sourced → outreach → reply → meeting |\n| **Time saved per operator** | >10 hours/week | VC self-reports | ROI justification; sourcing drops from 2-3 hrs/day to 30 mins |\n| **NPS from design partner** | >8/10 | Survey Week 8 | Qualitative signal: would they use this daily? Recommend it? |\n\n**Pass/Fail threshold:**\n- If reply rate <5% OR scoring accuracy <60% OR design partner stops using by Week 8 → Pivot or kill\n- If reply rate 5-12% AND scoring >60% → Iterate further (good foundation, needs tuning)\n- If reply rate >15% AND scoring >70% AND meetings booked → Scale to next VC\n\n---\n\n## 2. USER STORIES (PRIORITIZED)\n\n### P0: Must Have for MVP\n\nThese are the stories that, if missing, the agent is broken or unusable.\n\n#### P0-1: Define Investment Thesis\n**User Story:** As a growth operator, I want to define a VC's investment thesis in natural language so the agent knows what to look for.\n\n**Acceptance Criteria:**\n1. Thesis is stored as structured configuration (JSON or simple key-value)\n2. Operator inputs thesis as plain English (e.g., \"Series A B2B SaaS in fintech, US/EU, $1-5M ARR, profitable or path to profitability\")\n3. Thesis is parsed into filterable criteria: stage, sector(s), geography, company size, profitability, other custom filters\n4. Accuracy test: run thesis against 50 known companies (25 portfolio companies = positive, 25 non-fits = negative). System achieves >70% precision + recall.\n5. Thesis can be edited without restarting pipeline\n\n**Success criteria:** Design partner confirms thesis is correctly understood by Week 2 EOD.\n\n---\n\n#### P0-2: Daily Twitter/X Signal Ingestion\n**User Story:** As a growth operator, I want the agent to ingest founder signals from Twitter/X daily so I have fresh leads.\n\n**Acceptance Criteria:**\n1. Cron job runs daily at 6 AM (configurable)\n2. Monitors keyword list (e.g., \"just launched\", \"raising Series A\", \"founded\") + account list (e.g., @ProductHunt, @TechCrunch, founders design partner specifies)\n3. Extracts: founder name, handle, company, tweet text, timestamp, follower count, URL/link if present\n4. Deduplication: if same founder tweeted twice, merge signals into one record\n5. Structured storage: tweets table with (founder_id, tweet_content, timestamp, source)\n6. Pipeline completes in <30 minutes for 500 accounts\n7. Captures 50+ new founders per day (KPI)\n\n**Success criteria:** Operator confirms recognizes 80%+ of sourced founders as \"real founders\" by Week 2.\n\n---\n\n#### P0-3: Product Hunt Launch Ingestion\n**User Story:** As a growth operator, I want the agent to ingest new Product Hunt launches so I catch emerging startups.\n\n**Acceptance Criteria:**\n1. Daily scrape of Product Hunt \"new\" and \"trending\" sections\n2. Extracts: founder name, company name, product description, upvote count, product URL, launch date\n3. Data merged into unified founder/company table\n4. Handles 5-20 new PH launches per day\n5. Timestamps allow tracking which founders were discovered via PH\n\n**Success criteria:** 30%+ of final sourced founders come from PH by Week 4.\n\n---\n\n#### P0-4: Thesis-Based Scoring Engine\n**User Story:** As a growth operator, I want each founder scored against the thesis so I can focus on the best matches.\n\n**Acceptance Criteria:**\n1. Scoring system: 0-100 scale\n2. Score is computed from: (a) founder signals + company description, (b) VC's stated thesis\n3. Uses Claude API (LLM-based reasoning)\n4. Output: score + brief explanation (1-2 sentences why this founder matches or doesn't)\n5. Scores cached (don't re-score the same founder without new signals)\n6. Re-scoring triggered when new signals arrive for a founder\n7. Batching: pipeline scores 200+ new founders per day within budget ($50-150/mo LLM)\n8. Scoring latency: <5 seconds per founder (human-reviewer won't wait longer)\n\n**Accuracy test (Week 3 checkpoint):** Design partner manually grades 50 founders as \"yes, I want to talk to this person\" or \"no way\". System achieves >70% match (e.g., if VC says \"yes\" to 15, system scores those 15 highest on average).\n\n**Success criteria:** Design partner confirms >70% accuracy on blind sample by Week 3 EOD.\n\n---\n\n#### P0-5: Daily Digest Dashboard\n**User Story:** As a growth operator, I want a daily digest dashboard showing top-scored founders so I can quickly see prospects.\n\n**Acceptance Criteria:**\n1. Web dashboard (accessible, single-page)\n2. Lists all founders scored today, sorted descending by score\n3. Per founder: name, company, score (prominent), top signal (Twitter quote or PH description), one-line summary\n4. Can filter by score threshold (e.g., show only >80 scores)\n5. Founder name is clickable link to detail view\n6. Dashboard loads in <2 seconds\n7. Updated daily when pipeline runs (or operator can manually trigger refresh)\n\n**Success criteria:** Operator can review top 50 prospects in <5 minutes by Week 3.\n\n---\n\n#### P0-6: AI-Generated Personalized Outreach Emails\n**User Story:** As a growth operator, I want AI-generated personalized outreach emails for each founder so I don't write from scratch.\n\n**Acceptance Criteria:**\n1. For each founder, system generates an email draft\n2. Draft is personalized: references specific founder signals (e.g., \"Saw your tweet about shipping this week\", \"Your Product Hunt launch got 500 upvotes\")\n3. Email tone/voice is configurable per VC (templates: e.g., \"warm & casual\" vs \"formal & direct\")\n4. Length: 75-150 words (conversational, not salesy)\n5. Includes a CTA: \"quick call?\" or \"demo?\" (configurable)\n6. Draft is stored in database, marked as \"pending approval\"\n7. Uses Claude Sonnet (better writing quality than Haiku)\n\n**Success criteria:** Design partner reviews 10 drafts, approves >50% without major edits by Week 4.\n\n---\n\n#### P0-7: Approval Queue & Email Sending\n**User Story:** As a growth operator, I want to approve/edit/reject email drafts before they send.\n\n**Acceptance Criteria:**\n1. Queue UI shows pending drafts (newest first)\n2. For each draft: founder name, score, generated email text, founder email address\n3. Operator can: (a) approve + send, (b) edit text + send, (c) reject + mark \"don't email this founder\"\n4. Approved emails sent via Postmark (transactional email service)\n5. Email headers: From (VC's email), Reply-To (VC's email), unsubscribe link (CAN-SPAM compliance)\n6. Sent emails logged with timestamp, recipient, content hash\n7. Rejections fed back to improve future drafts (optional for v1, but track for v2)\n\n**Success criteria:** 5+ emails sent to real founders by Week 4 EOD.\n\n---\n\n#### P0-8: Outreach Status Tracking (CRM-lite)\n**User Story:** As a growth operator, I want to track outreach status per founder so I know who replied and who I should follow up.\n\n**Acceptance Criteria:**\n1. Founder profile shows outreach state: New → Contacted → Replied → Meeting Booked → Passed\n2. Manual state transitions (operator updates when they get a reply or book a call)\n3. Timestamp on each state change\n4. History visible: \"contacted 2024-02-14, replied 2024-02-16\"\n5. Dashboard filters by state (e.g., show all \"Replied\" in last 7 days)\n\n**Success criteria:** Operator has manually transitioned >10 founders through states by Week 6.\n\n---\n\n#### P0-9: Deduplication\n**User Story:** As a growth operator, I want the agent to deduplicate founders across sources so I don't email the same person twice.\n\n**Acceptance Criteria:**\n1. Matching logic: fuzzy match on name + company name (handle edge cases: John Smith @ Acme Inc vs J. Smith @ Acme Industries)\n2. When duplicate detected: merge signal records (keep both Twitter tweet and PH launch in history, single founder record)\n3. No duplicate outreach: if founder already has state=Contacted or Replied, skip them in tomorrow's sourcing\n4. Accuracy: <1% false negatives (miss duplicates) on test set of 500 founders\n\n**Success criteria:** Zero duplicate emails sent by Week 6.\n\n---\n\n#### P0-10: Configurable Data Sources\n**User Story:** As a growth operator, I want to configure which Twitter accounts and keywords to monitor so I can customize sourcing to my thesis.\n\n**Acceptance Criteria:**\n1. Admin UI to add/remove:\n   - Twitter keywords (e.g., \"raising\", \"just launched\", \"Series A\")\n   - Twitter accounts to monitor (e.g., @ProductHunt, @YCombinator, founder accounts)\n   - Crunchbase filters (optional, but allow disabling)\n2. Changes take effect on next scheduled pipeline run (next day 6 AM)\n3. Configuration stored in database, editable per VC\n\n**Success criteria:** Design partner can adjust keyword list by Week 2.\n\n---\n\n### P1: Important, Can Ship Without (but should try)\n\n#### P1-11: Crunchbase Enrichment\n**User Story:** As a growth operator, I want Crunchbase enrichment for each founder (funding history, company details) to understand context.\n\n**Details:** Query Crunchbase free/API tier for company funding stage, raised amount, investor list, company website. Enrichment makes scoring context richer.\n\n#### P1-12: Scoring Explainability\n**User Story:** As a growth operator, I want to see why the agent scored a founder high (explainability) so I can debug poor scores.\n\n**Details:** When viewing a founder detail page, show the LLM's reasoning: \"Matches thesis on: Series A stage (confirmed via Crunchbase), SaaS product, EU-based. Doesn't match on: not in fintech (in HR tech instead).\"\n\n#### P1-13: Weekly Performance Report\n**User Story:** As a growth operator, I want weekly performance reports (leads sourced, emails sent, replies received) so I can measure ROI.\n\n**Details:** Auto-generated email every Monday with: sourced founders this week, emails sent, reply count, reply rate, meetings booked.\n\n#### P1-14: Founder Snoozing\n**User Story:** As a growth operator, I want to snooze a founder and revisit them later.\n\n**Details:** Mark founder as \"snoozed until 2024-03-15\". Pipeline skips them until that date.\n\n#### P1-15: Email Open/Click Tracking\n**User Story:** As a growth operator, I want email open/click tracking to gauge interest.\n\n**Details:** Postmark provides this via webhooks. Track: opened_at, clicked_at, click_url. Display on founder detail page.\n\n#### P1-16: Founder Tags/Labels\n**User Story:** As a growth operator, I want to tag founders with custom labels (e.g., \"hot lead\", \"portfolio fit\", \"follow up in March\") so I can organize my pipeline.\n\n**Details:** Multi-select tags per founder record.\n\n#### P1-17: Real-Time Tweet Detection\n**User Story:** As a growth operator, I want the agent to detect when a founder I'm tracking tweets something new.\n\n**Details:** Real-time webhook from Twitter (requires Twitter API premium). Lower priority than daily batch ingest for MVP.\n\n---\n\n### P2: Nice-to-Have / Post-MVP\n\n#### P2-18: Scoring Learning\nAs a growth operator, I want the agent to learn from my approve/reject patterns to improve scoring.\n\n#### P2-19: Slack Notifications\nAs a growth operator, I want Slack notifications for high-score founders (>85) so I don't miss hot leads.\n\n#### P2-20: Import Existing Contacts\nAs a growth operator, I want to import a CSV of existing contacts to avoid re-outreach.\n\n#### P2-21: Multi-Sequence Drip Campaigns\nAs a growth operator, I want follow-up emails in a sequence so I don't lose touch.\n\n#### P2-22: Sourcing Performance Comparison\nAs a growth operator, I want to compare sourcing performance across different thesis configurations.\n\n#### P2-23: HackerNews Integration\nAs a growth operator, I want the agent to monitor HackerNews \"Show HN\" posts.\n\n#### P2-24: Calendar Integration\nAs a growth operator, I want auto-booking when a founder replies with availability.\n\n---\n\n## 3. EIGHT-WEEK SPRINT PLAN\n\n### Week 1-2: Foundation & Data Pipeline\n\n**Goal:** Founders are appearing in the database from real sources. Dashboard skeleton exists. Deduplication logic works. Team understands data flow.\n\n**Key Deliverables:**\n\n#### Week 1\n\n**Dev Infrastructure (Days 1-2)**\n- Project scaffolding: monorepo structure (e.g., `/apps/dashboard`, `/apps/backend`, `/jobs`)\n- CI/CD: GitHub Actions or similar (run linting + tests on every push)\n- Database: Supabase project created, schema initialized\n- Secrets: API keys for Twitter, Claude stored in `.env`\n- Dev environment: local Docker setup or cloud database tunnel\n\n**Schema Design (Days 1-2)**\n- Tables: `founders` (id, name, company, email, created_at), `signals` (id, founder_id, signal_type, content, timestamp, source), `thesis_configs` (id, vc_name, thesis_text, created_at), `outreach` (id, founder_id, state, email_sent_at, replied_at), `emails` (id, founder_id, draft_text, approved_at, sent_at)\n- Indexes on: founder_id (signals, outreach), timestamp (signals), state (outreach), score (founders)\n\n**Twitter/X Data Pipeline (Days 3-5)**\n- Twitter API v2 setup (apply for Basic tier access, $100/month)\n- Keywords to monitor: \"raising Series A\", \"just shipped\", \"founded\", \"YC cohort\" (design partner provides list by Day 2)\n- Accounts to monitor: @ProductHunt, @YCombinator, @TechCrunch, startup news accounts (start with 50, expand later)\n- Python script (using `tweepy` or `httpx` + Twitter API v2): \n  - Daily batch job fetches tweets matching keywords + from followed accounts\n  - Parses tweets, extracts potential founder signals (name, company mention, URL)\n  - Stores raw tweet + metadata in `signals` table\n- Deduplicates by (author_handle, tweet_date)\n- Logs: 500+ tweets ingested per day (KPI), zero API errors\n\n**Product Hunt Scraper (Days 5-7)**\n- RSS feed parser: Product Hunt publishes RSS for \"new\" and \"trending\"\n- Daily scrape via cron (6 AM)\n- Extracts: product name, founder/maker name, tagline, upvote count, URL, launch date\n- Stores in `signals` table with `source='product_hunt'`\n- Handles 10-20 launches per day\n\n**Deduplication Logic (Day 7)**\n- Fuzzy matching: Given a new founder signal, check if founder already exists\n- Matching rules: `(name_similarity > 0.85 AND company_similarity > 0.80) → same founder`\n- Use `fuzzywuzzy` library for string distance\n- Merges signals into single founder record\n\n**Testing & QA (Day 7)**\n- Manual: run scrapers, verify data in database (50+ founders, signals look correct)\n- No automated tests yet (will add Week 3)\n\n**By end of Week 1:**\n- 100+ founders in database from Twitter + PH\n- Zero data corruption\n- Pipeline runs without manual intervention\n- Design partner gets a read-only database dump to inspect data quality\n\n#### Week 2\n\n**Thesis Configuration & Scoring Scaffold (Days 1-3)**\n- Data model: `thesis_configs` table with (vc_name, thesis_text, filters JSON, created_at)\n- UI skeleton: form to input thesis. Example: \"Series A SaaS, $1-5M ARR, fintech, US/EU, post-revenue\"\n- Design partner fills in their actual thesis by Wednesday\n- Scoring function stub (takes founder + thesis → returns score 0-100, no LLM yet, just placeholder)\n\n**LLM Scoring Integration (Days 3-5)**\n- Claude API setup: use `anthropic` Python library\n- Scoring prompt design:\n  ```\n  You are a VC investment analyst. Evaluate this founder against the investment thesis.\n  \n  THESIS: {thesis_text}\n  \n  FOUNDER SIGNALS:\n  - Twitter: {twitter_summary}\n  - Product Hunt: {ph_summary}\n  - News: {any_crunchbase_data}\n  \n  Score this founder 0-100 based on fit. Provide score + one-line reasoning.\n  ```\n- Batch scoring: given list of 50 founders, score all in parallel (using Claude's batch API if available, else sequential with rate-limit handling)\n- Cache scores: if founder has no new signals since last scoring, reuse cached score\n- Cost tracking: log tokens used, ensure staying within $200/mo budget (estimated: 200 founders × 500 tokens = 100K tokens/day = ~$2.50/day)\n\n**Database & Caching (Days 4-5)**\n- Add `scores` table: (founder_id, thesis_id, score, explanation, scored_at, expires_at)\n- Cache invalidation: re-score if (now - scored_at) > 7 days OR new signals arrived\n\n**Testing (Days 5-6)**\n- Manual: score 20 founders against design partner's thesis\n- Design partner provides gold standard (yes/no on each)\n- Calculate precision (of the ones I scored \"yes\", how many did VC actually want?) and recall (of the ones VC wanted, how many did I find?)\n- Target: >60% accuracy. If below, iterate prompt.\n\n**Dashboard Skeleton (Days 5-7)**\n- Next.js app created with `create-next-app`\n- Pages: `/dashboard` (main feed), `/founder/[id]` (detail)\n- Auth: Supabase Auth (magic link or GitHub OAuth)\n- Feed page: fetches founders from Supabase API, displays as list with name, company, score\n- No styling beyond basic HTML yet (shadcn/ui will be added Week 3)\n- Data: fetched fresh on page load (no client-side caching yet)\n\n**By end of Week 2:**\n- 200+ founders in database from real sources\n- Scoring working on 20 founders with >60% accuracy\n- Dashboard loads and displays founders\n- One VC thesis configured and working\n- All code in Git with basic CI\n\n**Definition of Done (Week 1-2):**\n- [ ] 200+ real founders in database\n- [ ] Twitter + PH sources running daily without manual intervention\n- [ ] Deduplication working (no obvious duplicates)\n- [ ] Thesis config created for design partner\n- [ ] LLM scoring achieving >60% accuracy on 20-founder test\n- [ ] Dashboard displaying founders from database\n- [ ] All code committed, CI passing\n- [ ] No data corruption or data loss\n- [ ] Monitoring alerts set up (Sentry for errors, Slack for pipeline failures)\n\n**Dependencies:**\n- Twitter API access (apply Day 1, usually approved within 48 hours)\n- Claude API access (requires Anthropic account, instant)\n- Supabase project (free tier, instant)\n- Design partner available for thesis input (Week 1 Wednesday) and accuracy feedback (Week 2 Friday)\n\n**Risks & Mitigations:**\n\n| Risk | Probability | Mitigation |\n|------|-------------|-----------|\n| Twitter API free tier rate limits | High | Budget for Basic tier ($100/mo). Start with 50 accounts monitored, expand if no issues. Use Apify as fallback scraper ($49/mo). |\n| Scoring accuracy <60% | Medium | If failing, add more context to prompt (founder bio, company website, news articles). Provide few-shot examples (design partner gives 5 \"yes\" and 5 \"no\" examples). |\n| Product Hunt scraper breaks (site changes HTML structure) | Medium | Use RSS feed only (more stable). Fallback: manual daily check by operator. Build flexibility to disable sources. |\n| Database grows too fast (storage limits) | Low | Supabase free tier: 500MB. At 200 founders/day with signals, that's ~5-10 KB/founder = 48 GB/year. Migrate to paid tier in Week 4 if needed ($25/mo). |\n\n---\n\n### Week 3-4: Core Engine (Scoring & Outreach)\n\n**Goal:** Scoring is validated as >70% accurate. Email drafting works. Operator can approve and send emails. First real outreach campaign runs. CRM states working.\n\n**Key Deliverables:**\n\n#### Week 3\n\n**Scoring Accuracy Push (Days 1-5)**\n- Operator: score 100 founders manually (yes/no on each)\n- Dev: batch score same 100 with current prompt\n- Analysis: which ones did we get wrong? False positives (scored high, VC said no) vs false negatives (scored low, VC said yes)\n- Iteration: rewrite prompt to fix patterns. Examples:\n  - If falsely positive on \"non-founder CEO/founder hybrid roles\", add criteria: \"founder should be CTO or CEO, not VP of something\"\n  - If falsely negative on \"early-stage but will grow\", add: \"Series Pre-A or Seed with strong pedigree (YC alumni, ex-founders)\"\n- Test new prompt on same 100\n- Target: >70% accuracy\n- If stuck, escalate to: (a) few-shot prompting (give Claude 5 examples of \"yes\" and 5 of \"no\"), (b) ensemble scoring (score multiple times, average), (c) manual override for fuzzy cases\n\n**Email Draft Generation (Days 3-5)**\n- Prompt design:\n  ```\n  You are a VC analyst drafting a personalized outreach email to a founder.\n  \n  FOUNDER NAME: {name}\n  COMPANY: {company}\n  SIGNALS:\n  - Tweet: {tweet_text}\n  - Product Hunt: {ph_tagline}\n  - Score: {score}/100 ({reason})\n  \n  VC VOICE: {voice_template}\n  \n  Write a short, friendly email (75-150 words) that:\n  1. Opens with a specific signal (tweet or PH launch)\n  2. Explains why we're interested\n  3. Asks for a quick call or demo\n  4. Signs with {vc_name}\n  \n  Do not be salesy. Sound genuine.\n  ```\n- Voice templates: (a) \"warm & casual\" - conversational, emoji OK, (b) \"formal & direct\" - no emojis, professional\n- Generate draft for all 100 founders (or first 50 if budget-constrained)\n- Store drafts in `emails` table with (founder_id, draft_text, created_at, status='pending')\n\n**Email Quality Review (Days 5-7)**\n- Show 20 drafts to design partner\n- Grades each: 1 (terrible), 2 (needs heavy edits), 3 (good with minor tweaks), 4 (send as-is)\n- Acceptance rate: % graded 3 or 4. Target >50%.\n- Feedback: common issues (too formal? too salesy? didn't mention right signal?)\n- Iterate prompt if <50% acceptance\n\n**Email Sending Integration (Day 5-7)**\n- Postmark account setup (free tier: $0 first month, then $15/mo for 10K emails/mo)\n- Auth: store Postmark API token in Supabase secrets\n- Function: `send_email(to, from, subject, body, reply_to)` using Postmark Python library\n- Headers: Postmark auto-adds List-Unsubscribe and bounces/deliverability tracking\n- Test: send 3 emails to team members (qa + founder) to verify delivery, tracking\n\n**Outreach Approval Queue UI (Days 4-7)**\n- Next.js page `/dashboard/outreach/queue`\n- Lists pending email drafts (all with status='pending'), newest first\n- Per draft: founder name, company, score, draft text (editable inline), action buttons [Approve] [Reject] [Edit & Send]\n- Approve flow: (a) confirm send, (b) set status='approved', (c) call Postmark, (d) set status='sent', timestamp sent_at\n- Edit & Send: operator can modify draft text before sending\n- Reject: sets status='rejected', doesn't send, reason optional (for future learning)\n- Design: clean, mobile-friendly if possible but desktop priority\n- Styling: add shadcn/ui components (Card, Button, Input)\n\n**CRM Outreach Status (Days 5-7)**\n- Founder detail page updates to show outreach history\n- Fields: state (New/Contacted/Replied/Meeting/Passed), timestamps, draft text, sent email text\n- State transitions: operator can manually set (no automation yet)\n- Display: timeline of events\n\n**Testing & QA (Days 6-7)**\n- E2E: pick 10 founders, generate drafts, approve 5, reject 5, verify in dashboard\n- Manual: send 5-10 real emails to actual founders (or friendly test accounts)\n- Verify: emails arrive in inbox, open/click tracking works\n- No automated tests yet (will add in Week 5)\n\n**By end of Week 3:**\n- Scoring accuracy >70% on 100-founder test set\n- Email draft acceptance >50%\n- 5-10 real emails sent to founders\n- Approval queue working\n- Design partner confirms quality\n\n#### Week 4\n\n**Refinement & Scale (Days 1-5)**\n- Full batch: score all 200+ founders in database\n- Generate drafts for all\n- Operator approves/rejects in batches over 2 days\n- Send 20-30 emails to real founders\n- Track open/click events via Postmark webhooks (stored in database)\n\n**Webhook Setup for Email Events (Days 1-3)**\n- Postmark sends webhooks: (a) delivery (sent), (b) bounce (permanent/temporary), (c) open, (d) click, (e) complaint\n- Endpoint: `/api/webhooks/postmark`\n- Verifies webhook signature (Postmark provides hmac-sha256)\n- On event, updates `emails` table: opened_at, opened_count, clicked_at, clicked_urls\n- Prevents duplicate processing (idempotency key)\n\n**Reply Detection (Days 3-5)**\n- Manual for now (v1 doesn't do autonomous reply parsing)\n- Operator manually updates founder state to \"Replied\" in UI\n- Alternative (Optional): forward replies to Slack, operator updates state from there\n- Future (v2): email reply parsing + sentiment analysis\n\n**Founder Deduplication Refinement (Days 2-4)**\n- Review all 200+ founders for duplicates\n- Fuzzy match tuning: adjust similarity thresholds if too many false positives or false negatives\n- Merge detected duplicates\n- Verify zero duplicate outreach\n\n**CRM Polish (Days 4-5)**\n- Add bulk state transitions (select multiple founders, mark all as \"Passed\" in one click)\n- Add filters to founder list: by state, by score range, by source\n- Add search by name/company\n- Performance: pagination if >500 founders to avoid slow loads\n\n**Analytics Skeleton (Days 5-7)**\n- Track: (a) founders sourced per day, (b) emails sent, (c) opens, (d) clicks, (e) replies\n- Dashboard view (simple stats): total sourced, total sent, open rate, click rate\n- No charts yet, just numbers\n- Data: computed from database queries (not real-time, run daily cron)\n\n**Launch Checkpoint: Design Partner Review (Day 7)**\n- Demo: show dashboard, sorting/filtering, outreach queue, approval flow\n- Review: 20-30 emails sent + tracking data\n- Feedback: accuracy, relevance, time savings estimate\n- Go/no-go: does this solve your problem? Any deal-breakers?\n- Document: lessons for Week 5-6\n\n**By end of Week 4:**\n- 200+ founders scored >70% accuracy\n- 20-30 real emails sent\n- Email open/click tracking working\n- CRM states accurate\n- Founder list searchable, filterable, performant\n- Design partner actively using dashboard\n- Analytics skeleton in place\n\n**Definition of Done (Week 3-4):**\n- [ ] Scoring accuracy >70% on 100-founder gold standard\n- [ ] Email draft acceptance >50% (design partner review)\n- [ ] 20-30 real emails sent to founders\n- [ ] Postmark integration + webhook tracking working\n- [ ] Outreach approval queue fully functional\n- [ ] CRM state transitions working\n- [ ] Founder deduplication zero false positives\n- [ ] Dashboard performance: <2s load time\n- [ ] Design partner sign-off on quality + relevance\n\n**Dependencies:**\n- Design partner available for continuous feedback (3-4 check-ins this week)\n- Twitter + PH sources continuing to ingest data (Weeks 1-2 foundation must be solid)\n- Postmark account active + domain verified for sending\n\n**Risks & Mitigations:**\n\n| Risk | Probability | Mitigation |\n|------|-------------|-----------|\n| Email drafts still sound generic after iteration | Medium | Add 2-3 specific signal phrases to prompt. Show VC best examples + worst examples (few-shot). If still bad, may be data signal problem, not prompt. |\n| Reply rate <5% (poor outreach quality) | Medium | Don't chase this yet in Week 4. Focus on Week 5-6 testing at scale. 5% is acceptable for cold outreach. 15% is great but may take refinement. |\n| Founder emails not available (Hunter.io needed) | High | Integrate Hunter.io API ($49/mo, 5K lookups). Falls back to Twitter DM if email not found. Accept 30-40% of founders won't have findable emails. |\n| Postmark deliverability issues (emails in spam) | Medium | Check Postmark bounce/complaint rates daily. Warm up sending domain (send low volume for 2 weeks before scaling). Follow CAN-SPAM (unsubscribe link, headers). |\n\n---\n\n### Week 5-6: Automation & Analytics\n\n**Goal:** Daily pipeline runs fully autonomously. Operator opens dashboard once/day to approve drafts. Metrics prove the agent is working (reply rates, time saved). Scale to handle 200+ founders/day sustainably.\n\n**Key Deliverables:**\n\n#### Week 5\n\n**Full Automation Setup (Days 1-3)**\n- Daily cron job (6 AM):\n  1. Ingest Twitter + PH signals\n  2. Dedup against existing founders\n  3. Score new + updated founders\n  4. Generate email drafts\n  5. Notify operator (Slack or email: \"50 new drafts ready for approval\")\n  6. Update dashboard\n  7. Compute daily analytics\n- Inngest or APScheduler: schedule cron job\n- Error handling: if any step fails, send Slack alert with error message + retry logic\n- Dead-letter queue: failed drafts go to manual review queue\n- Idempotency: same founder scored twice on same day = only one draft generated\n\n**Signal Freshness (Days 2-4)**\n- New signals trigger re-scoring for that founder\n- Re-scored founders jump to top of approval queue (hot leads)\n- Notification: \"Founder XYZ just tweeted, score re-evaluated\"\n\n**Cost Optimization (Days 3-5)**\n- LLM usage: batch scoring with Claude Haiku (cheaper) for initial ranking, Sonnet (better quality) for top 50\n- Monitor daily spend: Anthropic API bills, Twitter API usage, Postmark email count\n- Set up alerts if approaching monthly budget caps\n- Target: <$200/mo LLM, <$100/mo email sending\n\n**Reply Detection Workflow (Days 4-5)**\n- Operator can manually mark founders as \"Replied\" in the UI\n- Dashboard shows \"Awaiting Response\" count (state='Contacted', no reply yet)\n- Alternative (if team capacity): Slack bot that allows `/reply-status {founder} {replied|meeting|passed}` for faster updates\n\n**Analytics Dashboard (Days 3-7)**\n- Page: `/dashboard/analytics`\n- Metrics displayed:\n  - **Sourcing:** founders added today, this week, total\n  - **Quality:** avg score, % scoring >70\n  - **Outreach:** drafts pending, sent this week, open rate, click rate\n  - **Engagement:** replies this week, meetings booked this week\n  - **Efficiency:** avg time to review/approve (goal: <1 min per draft)\n- Charts: line charts for daily trends (founders sourced, emails sent, reply rate)\n- Data refresh: daily (not real-time)\n- Filters: by score range, by source (Twitter vs PH)\n\n**Reporting (Day 6-7)**\n- Auto-generate weekly email report (every Sunday)\n- Content: sourcing summary, email stats, reply data, recommendation\n- Example: \"Sourced 350 founders this week. Sent 40 emails (avg score 78). Open rate 45%, reply rate 12%. Hot lead: XYZ Corp (score 94).\"\n\n**Testing & Performance Tuning (Days 5-7)**\n- Simulate 300+ founders in queue\n- Time each component: ingest (target <5 min), scoring (target <15 min), drafting (target <10 min), total <30 min\n- If too slow, optimize: parallel batch processing, caching, smaller batches\n- Database query optimization: add indexes, profile slow queries\n- Load test dashboard: 500+ founders, filters, search\n\n**By end of Week 5:**\n- Fully automated daily pipeline running\n- 300+ founders in database, continuously updated\n- 50+ emails sent this week\n- Reply data being tracked\n- Weekly reports auto-generated\n- Analytics dashboard useful\n\n#### Week 6\n\n**Quality Assurance & Stability (Days 1-3)**\n- Run pipeline 5 consecutive days without manual intervention\n- Verify: zero pipeline failures, zero data corruption, zero duplicate outreach\n- Monitor: logs, error rates, API rate limits\n- Alerts: set up Sentry for unhandled exceptions, Slack for job failures\n- Backup strategy: daily database backups to cloud storage (Supabase auto-handles, but document)\n\n**Founder Enrichment (Days 2-4)**\n- Optional: add Crunchbase API integration (free tier data: funding stage, raised amount, investor list)\n- Enrichment makes scoring context richer\n- Store in `signals` table or separate `enrichment` table\n- Add to scoring prompt: \"Company raised $2M Series A from Sequoia, founded 2022\"\n- Cost: free (Crunchbase free API has rate limits but sufficient for MVP)\n\n**Dashboard Refinement (Days 3-5)**\n- Add export: CSV export of founders (name, company, score, signals, email status)\n- Add bulk actions: select 10 founders, mark all as \"Passed\" in one click\n- Add search: find by founder name, company, or score range\n- Add sorting: by score, by date added, by status\n- UX polish: loading states, error messages, empty states\n\n**CRM Integrations (Optional, Days 5-6)**\n- Airtable sync (one-way): export approved/sent emails to Airtable base for VC partner if they want to view there\n- Alternative: HubSpot sync (if partner uses HubSpot, push new founders to \"Prospects\" list)\n- Requires 1-2 hours dev time per integration, defer if over budget on time\n\n**Performance Report to Design Partner (Day 7)**\n- Metrics snapshot:\n  - Founders sourced (Week 5-6): 350\n  - Emails sent: 40\n  - Opens: 18 (45%)\n  - Replies: 5 (12.5%)\n  - Meetings booked: 1 (on track)\n  - Time saved per operator: estimate 5-10 hrs/week\n- Feedback session: what's working? What needs improvement?\n- Iterate: adjust keyword list, thesis, email tone based on feedback\n\n**By end of Week 6:**\n- Fully stable, autonomous pipeline\n- 5+ consecutive days zero downtime\n- 300+ founders, 50+ emails sent, 5+ replies, 1+ meetings\n- All KPIs measurable and tracked\n- Design partner using system daily\n- Analytics showing value\n\n**Definition of Done (Week 5-6):**\n- [ ] Pipeline runs daily without manual intervention\n- [ ] Reply rate measurable (target >10%)\n- [ ] Time saved quantified (target >5 hrs/week per operator)\n- [ ] Analytics dashboard complete and useful\n- [ ] Weekly reports auto-generated\n- [ ] Founder enrichment integrated (Crunchbase)\n- [ ] Zero data corruption\n- [ ] Performance: pipeline <30 min, dashboard <2s load time\n\n**Dependencies:**\n- All Week 3-4 outputs working reliably (scoring, email generation, sending)\n- Design partner providing reply/meeting data\n- Cloud monitoring (Sentry, Slack) set up\n\n**Risks & Mitigations:**\n\n| Risk | Probability | Mitigation |\n|------|-------------|-----------|\n| Reply rate <10% | Medium | Don't panic. Cold email baseline is 5-8%. Focus on signature personalization. Test different email templates. If truly <5%, may indicate bad data quality, not bad outreach. |\n| Pipeline breaks silently (jobs don't run) | Low | Set up Inngest + Slack notifications. Dead-letter queue for failed jobs. Daily health check: if no new founders scored, alert operator. |\n| LLM costs exceed budget | Low | Monitor daily. Use Haiku more, Sonnet less. Cache aggressively. Set hard API spend caps. Current estimate: $2-3/day, well under budget. |\n| Database performance degradation | Medium | Add indexes, profile slow queries, implement pagination (load 50 founders at a time, not all 300). Migrate to Supabase paid tier ($25/mo) if needed. |\n\n---\n\n### Week 7-8: Polish & Launch\n\n**Goal:** MVP is production-ready, all success metrics hit or close, decision made on next steps (scale, pivot, or kill). Documentation and handoff complete.\n\n**Key Deliverables:**\n\n#### Week 7\n\n**Bug Fixes & Design Partner Feedback (Days 1-4)**\n- Design partner uses system daily (Week 6 output)\n- Collect feedback: UI confusion, missing features, quality issues\n- Prioritize: only fix CRITICAL (data loss, security, broken flow) and HIGH (confusing UX, bad experience)\n- LOW priority items (cosmetic, nice-to-have) → backlog for v1.1\n- Complete by Day 4\n\n**Onboarding Wizard (Days 2-4)**\n- New VC sign-up should be <5 minutes to get first dashboard\n- Flow: (a) create account (Supabase Auth), (b) enter thesis in natural language, (c) add Twitter keywords/accounts, (d) connect email address, (e) see first batch of founders\n- UI: step-by-step form (no complex settings)\n- Help text on each field\n- Skip for MVP if design partner is only user (but build for next customers)\n\n**Data Export (Days 3-4)**\n- CSV export: founders (name, company, score, signals, email status, email sent, opened, replied)\n- JSON export: structured dump for VC's internal systems\n- Used by design partner to audit + share with team\n\n**Documentation (Days 4-7)**\n- **Operator Playbook:** \"How to use the GrowthOps Agent in 10 minutes\"\n  - Dashboard tour\n  - Reviewing daily digest\n  - Approving/rejecting emails\n  - Tracking replies\n  - Understanding scores\n- **System Architecture Diagram:** data flow, components, APIs\n- **API Reference:** for future integrations (Slack, Airtable sync, etc.)\n- **Troubleshooting Guide:** common issues + fixes\n- Format: markdown in Git repo, maybe hosted on simple docs site (README.md for MVP)\n\n**Security & Compliance Audit (Days 5-7)**\n- Code review: no hardcoded secrets, no SQL injection, no XSS\n- Data handling: personal data only used for founder sourcing, not sold, not shared\n- CAN-SPAM compliance: all emails have unsubscribe link, From/Reply-To headers correct\n- GDPR (if any EU data): consent + privacy notice (defer if not applicable for first customer)\n- API security: Postmark API key stored in Supabase secrets (not in code), Supabase RLS policies set\n- Backups: Supabase auto-backups enabled\n\n**By end of Week 7:**\n- Zero critical bugs from design partner feedback\n- Onboarding wizard working (or documented as deferred)\n- Full documentation + API reference\n- Security audit passed\n- Design partner ready for final review\n\n#### Week 8\n\n**Final Metrics Review & Go/No-Go Decision (Days 1-3)**\n- Measure all success metrics from Section 1:\n\n| Metric | Target | Actual (Week 8) |\n|--------|--------|-----------------|\n| Founders sourced per day | 50+ | ___ |\n| Thesis match accuracy | >70% | ___ |\n| Email draft acceptance | >50% sent without edits | ___ |\n| Reply rate | >15% | ___ |\n| Meetings booked (Weeks 7-8) | 5+ | ___ |\n| Time saved per operator | >10 hrs/week | ___ |\n| Design partner NPS | >8/10 | ___ |\n\n- Pass criteria:\n  - **Green (Continue & Scale):** reply rate >12% AND meetings >1/week AND design partner NPS >8\n  - **Yellow (Iterate):** reply rate 5-12% AND scoring >70% → spend 2 more weeks tuning\n  - **Red (Pivot/Kill):** reply rate <5% AND scoring <60% AND design partner stops using → evaluate if problem is solvable\n\n**Performance Optimization (Days 2-3)**\n- Profile critical paths: what takes the longest? (Expect: LLM scoring, data ingestion)\n- Optimize: batch LLM calls, reduce API calls, cache more aggressively\n- Target: pipeline finishes in <20 minutes (currently ~30)\n- Load test: can system handle 2-3 concurrent VCs? (future multi-tenant test)\n\n**Demo & Presentation (Days 4-5)**\n- Record 5-minute demo video:\n  1. Dashboard view of today's founders\n  2. Clicking into a founder detail (signals, score explanation)\n  3. Approving an email in the queue\n  4. Checking analytics\n  5. Showing weekly report\n- Create 1-page summary: problem + solution + metrics + next steps\n- Use in pitch deck for next VC customer or investors\n\n**Launch Readiness Check (Days 5-6)**\n- Monitoring: Sentry alerts set up, Slack notifications working\n- Backup: database backups running daily, tested restore (do it once)\n- Documentation: playbook, FAQ, troubleshooting\n- Support: who answers emails from design partner at 3 AM? (defer to v1.1, but identify owner)\n\n**Next Steps Decision (Day 7)**\n- Based on metrics review, decide: \n  - **Scale:** deploy to 2nd VC customer, start sales outreach\n  - **Iterate:** spend 2 weeks on specific improvements (better email tone, add data source)\n  - **Pivot:** problem statement was wrong, try different angle\n  - **Kill:** unsolvable with current approach\n- Document decision + reasoning + next phase plan\n\n**Post-MVP Planning (Days 5-8)**\n- **v1.1 (Weeks 9-10, if scaling):** multi-tenant, self-service onboarding, Slack integration, LinkedIn source\n- **v2 (Months 3-4):** video integration, autonomous sending, marketplace (founders can \"subscribe\" to alerts)\n- **Go-to-market:** how to acquire next 10 VC customers? Partnerships, direct outreach, community?\n\n**By end of Week 8:**\n- All metrics measured\n- Go/no-go decision documented\n- Demo video + summary for stakeholders\n- Next phase plan clear\n\n**Definition of Done (Week 7-8):**\n- [ ] All P0 stories complete and tested\n- [ ] Design partner using system daily with satisfaction\n- [ ] Success metrics measured and documented\n- [ ] Security audit passed\n- [ ] Documentation complete (playbook, API reference)\n- [ ] Go/no-go decision made with data\n- [ ] Next phase (scale/iterate/pivot) planned\n\n**Risks & Mitigations:**\n\n| Risk | Probability | Mitigation |\n|------|-------------|-----------|\n| Metrics miss targets (reply rate <10%, meetings 0) | Medium | Don't treat as failure. Identify which metric failed. If reply rate low, may be data quality or email tone. Iterate Weeks 9-10 before giving up. |\n| Design partner disengages before Week 8 | Low | Weekly check-ins, show value early. If disengaging, find backup VC contact immediately. |\n| Security/compliance issues discovered Week 8 | Low | Run audit Week 6, not Week 8. Consult lawyer Week 7 if concerns. |\n| Burnout (small team, crunch schedule) | High | No weekend work. Reduce scope if behind on Week 7. Health > perfect product. |\n\n---\n\n## 4. TECH DECISIONS MATRIX\n\n### a) Database: PostgreSQL (via Supabase) vs. Airtable vs. Raw Postgres\n\n| Criterion | Supabase (Postgres) | Airtable | Raw Postgres (self-hosted) |\n|-----------|-------------------|---------|--------------------------|\n| **Setup Time** | 10 min (free tier) | 5 min | 1-2 hrs (Docker) |\n| **Record Limits** | 500MB free, then $25/mo | 1,000 records free, then $20/mo (Plus) | Unlimited (your cost) |\n| **Query Flexibility** | Full SQL, joins, aggregations | Limited filtering, no real joins | Full SQL |\n| **API Quality** | RESTful + GraphQL auto-generated, excellent | REST API but rate-limited, slow | Build your own or use PostgREST |\n| **Auth Integration** | Built-in Supabase Auth | No auth layer | Manual auth |\n| **Real-time Capability** | Supabase Realtime (WebSockets) | No real-time | PostgreSQL LISTEN/NOTIFY |\n| **Data Export** | Easy (CSV, JSON) | Difficult (API bottleneck) | Easy |\n| **Vendor Lock-in** | Medium (but PostgreSQL is standard) | High (difficult to migrate) | Low |\n| **Cost @ Scale** | $25-100/mo | $20-300/mo | $10-50/mo + ops overhead |\n| **Data Ownership** | Supabase hosted (EU available) | Airtable cloud only | Your server |\n\n**Recommendation: Supabase (PostgreSQL)**\n\n**Justification:**\n1. **Record limit:** Airtable's 1,000-record free tier is a hard blocker. At 200 founders/day, you hit that in 5 days. Supabase's 500MB is generous for MVP.\n2. **Price:** Both free tiers exist. Supabase paid ($25/mo) is same as Airtable Plus ($20/mo), but you get a real database instead of a spreadsheet.\n3. **Developer experience:** Supabase provides auto-generated REST API (no backend coding needed for basic CRUD), Postgres console, and easy backups. Perfect for small team.\n4. **Future-proofing:** You can migrate from Supabase to AWS RDS later without pain (same PostgreSQL). Migrating from Airtable is a nightmare.\n5. **Ecosystem:** Python + Postgres is a battle-tested combination. Libraries like `psycopg2`, `sqlalchemy` are mature.\n\n**Cost:** $0-25/mo (free tier, then $25/mo for paid)\n\n---\n\n### b) Backend: Python (FastAPI) vs. Node.js (Express) vs. Both\n\n| Criterion | Python (FastAPI) | Node.js (Express/Hono) | Both (Microservices) |\n|-----------|-----------------|----------------------|----------------------|\n| **LLM Ecosystem** | Excellent (langchain, litellm, anthropic) | Good but less mature | Best but overkill for MVP |\n| **Data Pipeline** | Excellent (pandas, dbt, polars) | Okay (but JS devs struggle) | Best but complex |\n| **Async Support** | Good (asyncio) | Excellent (native async/await) | Best but split complexity |\n| **Scraping & Crawling** | Excellent (BeautifulSoup, Scrapy) | Good (Cheerio, Playwright) | Overkill |\n| **Development Speed** | Very fast (dynamic typing, batteries included) | Very fast (huge npm ecosystem) | Medium (more integration needed) |\n| **Deployment** | Railway, Render (excellent support) | Vercel, Railway, Heroku | Railway or AWS (more ops) |\n| **Team Hiring** | Medium (Python devs available) | Large (most devs know JS) | Hard (need both) |\n| **Maintenance Cost** | Low (one runtime, one package manager) | Low (one runtime) | High (two runtimes, sync issues) |\n| **Startup Culture Fit** | Very high (most startups use Python for backends) | High (many use Node) | Low (splits team focus) |\n\n**Recommendation: Python (FastAPI)**\n\n**Justification:**\n1. **Core competency:** The agent's value is in data processing (scraping, scoring, enrichment) and LLM integration. Python dominates both.\n2. **LLM libraries:** langchain, litellm, anthropic SDK are all mature in Python. Node.js equivalents exist but lag.\n3. **Data pipelines:** Pandas, Polars, SQLAlchemy make data processing ergonomic. Node devs struggle here.\n4. **Single language, single runtime:** For a small team, reducing cognitive overhead is huge. One language = fewer context switches = fewer bugs.\n5. **FastAPI:** Modern, fast, auto-generates OpenAPI docs, excellent async support for webhooks.\n\n**Not using Node.js because:** The real work is LLM + data, not real-time web. Webhooks (Postmark, Twitter) are infrequent and easy to handle in FastAPI's async.\n\n**Not using both because:** Two runtimes mean two deployment pipelines, two dependency managers, two teams. At this stage, you can't afford it.\n\n**Cost:** Free (open-source)\n\n---\n\n### c) Frontend: Next.js vs. Remix vs. Streamlit/Retool\n\n| Criterion | Next.js | Remix | Streamlit | Retool |\n|-----------|---------|-------|-----------|--------|\n| **Time to Dashboard** | 3-5 days (scaffold + components) | 3-5 days (similar to Next.js) | 1-2 days (super fast) | 2-3 hours (UI builder) |\n| **Customization** | Unlimited | Unlimited | Limited (dropdown layouts) | Limited (pre-built components) |\n| **Professional Look** | High (with shadcn/ui) | High (same as Next.js) | Medium (looks like a prototype) | High (polished pre-built) |\n| **Future Scalability** | Excellent (full-stack React) | Excellent (similar to Next.js) | Poor (hard to extend) | Medium (limited to Retool's UI) |\n| **Cost** | Free (host on Vercel free) | Free (host on Remix Stacks) | Free | $10/mo |\n| **Learning Curve** | Medium (React + Next.js concepts) | Medium (similar to Next.js + Remix paradigms) | Low (just Python) | Very low (visual builder) |\n| **Team Fit** | Best if you have React experience | Best if you have React experience | Best if Python-only team | Best if zero frontend experience |\n| **Hosting** | Vercel (free tier) | Remix Stacks (free tier) | Streamlit Cloud (free) | Retool Cloud (paid) |\n\n**Recommendation: Next.js**\n\n**Justification:**\n1. **Feature set:** Approval queue, inline editing, real-time filtering, bulk actions → these require interactivity that Streamlit/Retool struggle with.\n2. **Professional appearance:** VCs expect a polished dashboard, not a Streamlit prototype. Next.js + shadcn/ui gets you there in same time as Retool UI builder.\n3. **Hiring:** Every developer has React experience. Streamlit is Python-only (limits hiring). Retool is vendor-locked.\n4. **Future-proofing:** If you want a founder-side portal, team dashboard, or partner integrations, Next.js gives you unlimited flexibility. Retool + Streamlit hit ceilings fast.\n5. **Cost:** Free deployment on Vercel (up to limits).\n\n**Alternative: If your team has zero React experience and wants to ship fastest → Retool (2-3 hours to functional dashboard). But you'll outgrow it by Week 6.**\n\n**Cost:** Free (Vercel free tier, shadcn/ui free, Next.js free)\n\n---\n\n### d) Email: SendGrid vs. Postmark vs. AWS SES\n\n| Criterion | SendGrid | Postmark | AWS SES |\n|-----------|----------|----------|---------|\n| **Deliverability (Inbox Rate)** | 94-96% (declining reputation) | 97-99% (best-in-class) | 95-97% (good, requires warm-up) |\n| **Setup Time** | 10 min | 5 min | 30 min (SPF/DKIM/DMARC) |\n| **Free Tier** | 100/day (first 30 days) | None (but $15/mo cheap) | 62K/month (first 12 months) |\n| **Pricing** | $20-350/mo | $15-300/mo (10K emails = $15) | $0.10 per 1K emails (~$1-5/mo at MVP volume) |\n| **Webhook Quality** | Good (event data clear) | Excellent (detailed, reliable) | Good (but requires SNS setup) |\n| **Analytics** | Basic (opens, clicks, bounces) | Excellent (segmentation, trends) | Minimal (bounces, complaints only) |\n| **SMTP Relay** | Yes | Yes | Yes |\n| **Ease of Use** | Good (web UI) | Excellent (simple API) | Moderate (AWS console can be complex) |\n| **Support** | Good | Excellent | Good (AWS documentation) |\n\n**Recommendation: Postmark**\n\n**Justification:**\n1. **Deliverability:** Cold email to founders is all about landing in inbox, not spam. Postmark has the best reputation. 2-3% difference in deliverability = 2-3 extra replies per 100 emails = 20-30% more meetings.\n2. **No free tier (but cheap):** $15/mo is negligible vs. the risk of emails bouncing.\n3. **Webhook quality:** For tracking opens/clicks (KPI metric), Postmark's webhook payloads are the cleanest to parse.\n4. **SMTP + API:** Supports both, so you're not locked into their API.\n\n**Why not SES?** Requires warm-up period (reputation building), and savings ($1 vs $15/mo) are irrelevant. Postmark's deliverability advantage is worth 100x the cost difference.\n\n**Why not SendGrid?** Reputation has been declining due to abuse. Not worth the risk for cold outreach.\n\n**Cost:** $15/mo (10K emails/mo, well above MVP needs)\n\n---\n\n### e) LLM: Claude vs. GPT-4o vs. Mix\n\n| Criterion | Claude (Haiku/Sonnet/Opus) | GPT-4o / 4o-mini | Mix (Claude + OpenAI) |\n|-----------|---------------------------|-----------------|----------------------|\n| **Scoring (Complex Reasoning)** | Sonnet: excellent | 4o: good | Sonnet for scoring, 4o-mini for simple tasks |\n| **Email Generation (Writing Quality)** | Sonnet: very good (nuanced) | 4o: good (sometimes formulaic) | Sonnet for emails, mini for bulk |\n| **Speed** | Haiku: fast, Sonnet: medium | 4o-mini: fastest, 4o: slower | Mix: varies |\n| **Cost (per 1M tokens)** | Haiku: $0.25, Sonnet: $3 | mini: $0.15, 4o: $2.50 | Varies |\n| **Context Window** | Sonnet: 200K (huge) | 4o: 128K | Both available |\n| **Function Calling** | Good | Excellent | Mixed experience |\n| **Rate Limits (Free Trial)** | Very generous (Anthropic credits) | Generous (OpenAI free trial) | Combined generous |\n| **API Stability** | Excellent (very reliable) | Excellent (very reliable) | Same as individuals |\n| **Vendor Lock-in** | Medium (standard API, easy to switch) | Medium (standard API, easy to switch) | Low (can use either) |\n\n**Recommendation: Claude (Tiered)**\n\n**Justification:**\n1. **Cost efficiency:** Use Haiku (cheap) for bulk scoring of all founders. Use Sonnet (better) for email generation and re-ranking top 50 candidates.\n   - 200 founders/day × 500 tokens/founder × $0.25/1M tokens = $2.50/day (Haiku scoring)\n   - 50 top founders × 800 tokens/email × $3/1M tokens = $0.12/day (Sonnet emails)\n   - **Total: ~$2.60/day = $78/mo** (well under budget)\n\n2. **Writing quality:** Claude Sonnet's email generation is noticeably better than GPT-4o-mini. Founder emails are your product; worth the extra cost.\n\n3. **Single provider:** One API key, one billing, one support channel. Simpler ops.\n\n4. **Reasoning for scoring:** Claude's system prompts + extended thinking capability is slightly better for complex thesis matching.\n\n**Not using GPT only because:** Claude Sonnet's writing is slightly better for cold email (less generic).\n\n**Not mixing because:** Operational overhead not worth the ~10% cost savings. One provider scales cleaner.\n\n**Cost estimate:** $75-150/mo (bulk Haiku + top-tier Sonnet, well under budget)\n\n---\n\n### f) Hosting: Vercel vs. Railway vs. AWS vs. Self-hosted\n\n| Criterion | Vercel | Railway | AWS (ECS/Lambda) | Self-hosted (VPS) |\n|-----------|--------|---------|------------------|------------------|\n| **Frontend Hosting** | Excellent (optimized for Next.js) | Good | Possible (CloudFront) | Possible (Nginx) |\n| **Backend Hosting** | Good (Serverless Functions) | Excellent (VMs, cron) | Excellent (infinitely scalable) | Good |\n| **Cron Jobs** | Limited (paid tier) | Excellent (included) | Lambda: good, complex setup | Easy (cron binary) |\n| **Database Integration** | Integrates with Supabase | Easy (direct env vars) | Easy (RDS) | Your responsibility |\n| **Setup Time** | 5 min (git push) | 15 min | 1-2 hrs (CloudFormation, IAM) | 30 min (SSH, configure) |\n| **Cold Starts** | ~1s (for serverless functions) | None (persistent VM) | 1-10s (Lambda) | None (persistent VM) |\n| **Cost** | Free tier: 100 GB bandwidth, 12 serverless function hours | Free tier: $5 credits then $5-20/mo | Free tier: 750 hours/mo, then ~$10-50/mo | $5-15/mo (Linode, DigitalOcean) |\n| **Scalability** | Excellent (serverless auto-scale) | Good (manual scale or auto) | Excellent (auto-scale, Spot instances) | Manual (upgrade VM size) |\n| **Monitoring** | Built-in (Vercel Analytics) | Good (Railway dashboard) | Built-in (CloudWatch) | Self-serve (Prometheus, Datadog) |\n| **Database Backups** | N/A (Supabase handles) | N/A (Supabase handles) | N/A (Supabase handles) | Manual |\n| **Vendor Lock-in** | Medium (Vercel serverless specific) | Low (standard Docker) | High (AWS IAM, services) | None |\n\n**Recommendation: Vercel (Frontend) + Railway (Backend)**\n\n**Justification:**\n1. **Split architecture:**\n   - Vercel: Next.js dashboard (what it's optimized for) → free tier sufficient\n   - Railway: Python backend + cron jobs (what it's good at) → $5-20/mo\n\n2. **Simplicity:** No AWS complexity (IAM, CloudFormation, RDS, ECS). Railway is \"Heroku for 2024\" — push Docker image, it just works.\n\n3. **Cost:** $5-25/mo total. AWS would be cheaper at scale but adds 4-6 weeks of ops overhead for MVP.\n\n4. **Cron jobs:** Railway includes cron scheduling (for daily pipeline). Vercel requires paid tier. AWS Lambda requires code + CloudWatch Events.\n\n5. **GitHub integration:** Both Vercel and Railway auto-deploy on git push. MVP moves faster with this.\n\n**Migration path:** If you outgrow Railway (unlikely in 8 weeks), migrate to AWS ECS (same Docker images, just orchestrated differently).\n\n**Cost:** Free to $20/mo (Vercel free + Railway $5-20/mo)\n\n---\n\n### g) Job Queue / Orchestration: Inngest vs. BullMQ vs. Celery vs. Simple Cron\n\n| Criterion | Inngest | BullMQ | Celery | Simple Cron + DB |\n|-----------|---------|--------|--------|-----------------|\n| **Language Support** | Any (HTTP-based) | Node.js only | Python only | Python only |\n| **Scheduling (Cron)** | Excellent (built-in) | Good (need separate job) | Good (Celery Beat) | Native (`APScheduler`) |\n| **Retries** | Automatic (configurable) | Manual (requires code) | Automatic | Manual |\n| **Dead-letter Queue** | Automatic (failed jobs) | Automatic | Manual | Manual |\n| **Monitoring** | Excellent UI (free tier) | Limited (web UI) | Limited (Flower) | None (build yourself) |\n| **Observability** | Logs visible in dashboard | Need to query Redis | Need to query Celery | Need to query database |\n| **Setup Time** | 5 min | 30 min (Redis setup) | 1 hr (Redis + Celery config) | 15 min |\n| **Infrastructure Needed** | None (serverless) | Redis + Node.js server | Redis/RabbitMQ + Python server | Database only |\n| **Cost** | Free tier generous | Free (Redis, $15-20/mo) | Free (broker cost: $15-20/mo) | Free (just database) |\n| **Scaling** | Infinite (serverless) | Limited (Redis memory) | Excellent (distributed workers) | Limited (polling) |\n| **Complexity** | Very low | Medium | High | Low |\n\n**Recommendation: Inngest (if using Railway/Vercel) OR Simple Cron + DB (if minimizing dependencies)**\n\n**Justification:**\n\n**Option A: Inngest (Recommended for MVP):**\n- **Why:** Zero infrastructure. Create functions, Inngest handles scheduling, retries, dead-letters, monitoring.\n- **Setup:** Connect GitHub, write Python function, deploy. Takes 10 minutes.\n- **Free tier:** 10K function invocations/month. MVP needs ~10 daily cron runs × 30 days = 300 invocations. Infinitely under limit.\n- **Future:** If scaling to 50 VCs, might hit free tier limits. Then upgrade ($50/mo) or move to Celery. But by then you have revenue.\n- **DX:** Inngest's web UI is the cleanest job monitoring experience.\n\n**Option B: Simple Cron + Database Queue (if minimizing external dependencies):**\n- **Why:** Python native, no external services, easy to understand.\n- **Setup:** Use `APScheduler` library, store job state in Supabase.\n- **Flow:** 6 AM cron runs → marks job as \"running\" → pipeline executes → marks \"completed\" or \"failed\"\n- **Downsides:** No automatic retries (need to code manually), no monitoring UI (need to query database), fragile (if job crashes, no alerting).\n- **Best if:** You want zero external services and don't mind rough edges.\n\n**Verdict:** Use Inngest for MVP. If it becomes a blocker (cost or vendor concern), switch to Celery (takes 2-3 days). Simple cron + DB is too fragile for production.\n\n**Cost:** Inngest free tier (sufficient for MVP)\n\n---\n\n### h) CRM: Build Custom vs. Airtable vs. HubSpot\n\n| Criterion | Build Custom (Supabase) | Airtable | HubSpot |\n|-----------|------------------------|----------|---------|\n| **Setup Time** | 1-2 days (schema + UI) | 1 hour | 2 hours (setup, integrations) |\n| **Cost** | Free (part of Supabase) | $20/mo (Plus tier, 5K records) | Free tier (500 contacts) → $50/mo (Professional) |\n| **Customization** | Unlimited | Limited (can't add custom fields easily) | Limited (HubSpot opinionated) |\n| **Data Ownership** | Yours (PostgreSQL) | Airtable's servers | HubSpot's servers |\n| **Integration Ease** | Easy (your own API) | Moderate (Airtable API, slow) | Excellent (everything connects) |\n| **Email Integration** | Manual (Postmark webhooks) | Manual (Airtable doesn't receive emails natively) | Automatic (HubSpot integration) |\n| **Reporting** | DIY (SQL queries) | Basic (Airtable views) | Excellent (built-in dashboards) |\n| **Scaling** | Unlimited | Slow at scale (record limits) | Unlimited |\n| **Team Collaboration** | Moderate (Supabase doesn't have real-time multi-user editing) | Excellent (Airtable is collaborative by design) | Excellent (designed for teams) |\n\n**Recommendation: Build Custom in Supabase**\n\n**Justification:**\n1. **Your needs are simple:** founder record, outreach status (New/Contacted/Replied/Meeting/Passed), timestamps, email history. Not a complex sales pipeline.\n2. **Same database:** Dashboard already reads from Supabase. Building CRM in Supabase keeps everything in one place, zero sync problems.\n3. **No record limits:** Airtable's 5K-record limit for Plus tier is okay now but a liability later. Supabase has no limits.\n4. **Cost:** Custom CRM is free (part of Supabase database layer you're already paying for).\n5. **Control:** Your data, your schema, no vendor lock-in.\n\n**Downside:** You build the UI yourself (but it's simple: founder list + detail page + state buttons, already mostly done for the dashboard).\n\n**HubSpot not needed because:** HubSpot's power is enterprise sales workflows (sequences, lead scoring, attribution). You're doing founder sourcing, not sales-to-enterprise. HubSpot is overkill + expensive ($50/mo Professional tier).\n\n**Airtable not needed because:** Can use it, but you're paying $20/mo for something you can build free in Supabase. And you'll hit record limits.\n\n**Cost:** Free (included in Supabase)\n\n---\n\n## SUMMARY: Tech Stack\n\n| Component | Choice | Cost | Notes |\n|-----------|--------|------|-------|\n| **Database** | Supabase (PostgreSQL) | Free-$25/mo | Generous free tier, auto-generated API, built-in auth |\n| **Backend** | Python (FastAPI) | Free | Excellent LLM + data ecosystem |\n| **Frontend** | Next.js (Vercel) | Free | Professional dashboard, fast iteration |\n| **Email** | Postmark | $15/mo | Best deliverability for cold email |\n| **LLM** | Claude (Haiku + Sonnet tiered) | $75-150/mo | Cost-optimized, best writing quality |\n| **Job Orchestration** | Inngest | Free | Serverless, no infrastructure, excellent DX |\n| **Hosting (Backend)** | Railway | $5-20/mo | Best Python hosting, easy cron jobs |\n| **Data Enrichment** | Crunchbase (free API) + Hunter.io | $49/mo (Hunter) | Free Crunchbase, email finding API |\n| **Monitoring** | Sentry (free tier) + Slack | Free | Error tracking + alerts |\n| **Total Monthly Cost** | | $220-380/mo | Very affordable for a VC tool |\n\n---\n\n## 5. RESOURCE PLAN\n\n### What One Person Can Build in 8 Weeks\n\n**Profile:** One strong full-stack engineer (Python + basic React) with no distractions.\n\n**Deliverables:**\n- All P0 user stories (10 stories = all MVP functionality)\n- Most P1 stories (maybe 5-7 out of 7)\n- Testing: basic happy-path tests, no comprehensive coverage\n- Documentation: README + operator playbook, not polished\n\n**Feasibility:** Doable but **tight**. Critical to:\n- Use Supabase (saves building auth + REST API)\n- Use shadcn/ui (saves designing + coding UI components)\n- Sacrifice polish (functional > pretty)\n- Use Inngest (saves building job orchestration)\n- Limit to 3 data sources (Twitter, PH, Crunchbase enrichment)\n- Skip video generation, autonomous sending, multi-tenancy\n\n**Risk:** High burnout. Schedule will be back-to-back delivery. No buffer for unexpected issues (API changes, bugs). If one person gets sick, project stalls.\n\n**Success probability:** 60% hit all metrics, 30% hit most metrics, 10% miss critical ones.\n\n---\n\n### What Needs 2-3 People (Recommended)\n\n**Team:**\n- **Developer 1 (Backend/Integrations):** Python, FastAPI, APIs\n  - Weeks 1-2: Data ingestion (Twitter, PH), deduplication\n  - Weeks 3-4: LLM scoring, email generation, Postmark integration\n  - Weeks 5-8: Pipeline automation, monitoring, performance tuning\n\n- **Developer 2 (Frontend/Full-Stack):** React/Next.js, Supabase\n  - Weeks 1-2: Database schema review, dashboard skeleton\n  - Weeks 3-4: Founder detail page, approval queue UI, CRM states\n  - Weeks 5-8: Analytics dashboard, filtering/sorting, polish\n\n- **Operator/PM (Part-time, Weeks 3-8):** Growth, design partner liaison\n  - Design partner relationship management\n  - Outreach copy iteration + feedback\n  - Metric tracking, weekly reports\n  - QA (manual testing of scoring accuracy)\n\n**Parallel work schedule:**\n- Weeks 1-2: Minimal dependency (Dev 1 builds pipeline while Dev 2 scaffolds frontend)\n- Weeks 3-4: Medium dependency (Dev 1 generates drafts, Dev 2 builds UI to approve them)\n- Weeks 5-8: Low dependency (Dev 1 optimizes backend, Dev 2 polishes frontend)\n\n**Ideal timeline:** Deliver MVP + polish by Week 8, no crunch.\n\n**Success probability:** 85% hit all metrics, 15% minor misses.\n\n---\n\n### What to Outsource\n\n| Task | Provider | Cost | Why |\n|------|----------|------|-----|\n| **Logo + Brand Identity** | Fiverr / existing designer | $50-200 | Not core to MVP. Simple logo fine for Week 1. |\n| **Legal Review (CAN-SPAM, GDPR)** | Startup lawyer (1-2 hours) | $300-500 | Necessary but one-time. Outreach copy review. |\n| **Email Deliverability Setup** | Postmark docs (self-serve) | $0 | Excellent documentation. Dev can do it. |\n| **Twitter API Access** | Self-serve (Twitter Developer portal) | $100/mo | Just payment + signup. Takes 15 min. |\n| **Security Audit** | Internal (Week 7) | $0 | One developer review is sufficient for MVP. |\n\n**Note:** Do NOT outsource:\n- Data engineering (scraping, deduplication)\n- Backend development (scoring, email generation)\n- Frontend development (dashboard, UX)\n- LLM prompt engineering (core to value prop)\n\nThese are your product. Outsourcing gutts it.\n\n---\n\n### What to Use Off-the-Shelf\n\n| Need | Solution | Cost | Benefit |\n|------|----------|------|---------|\n| **Database** | Supabase (PostgreSQL) | Free-$25/mo | Never build database yourself. |\n| **Email Sending** | Postmark | $15/mo | Deliverability is hard. Use expert. |\n| **Auth** | Supabase Auth | Free | Never build auth yourself. |\n| **LLM API** | Anthropic (Claude) | $75-150/mo | Building LLM is not viable. Use API. |\n| **Error Tracking** | Sentry | Free | Observability essential for production. |\n| **Job Orchestration** | Inngest | Free | Don't build cron job infra. Use SaaS. |\n| **Email Finding** | Hunter.io | $49/mo | Email scraping is fragile + expensive. Use API. |\n| **Frontend Components** | shadcn/ui | Free | Pre-built, high-quality components. |\n| **Monitoring** | PostHog (free tier) | Free | Event tracking + analytics. |\n\n**Why:** Don't reinvent solved problems. You have 8 weeks to prove a new idea (founder sourcing agent). Spend time on that, not on building database infrastructure or auth systems.\n\n---\n\n### Estimated Burn Rate (People + Services)\n\n**2-person team:**\n\n| Item | Cost |\n|------|------|\n| **Salaries (outsourced, not internal)** | $0 (assume co-founders or VC-funded) |\n| **Infrastructure (Supabase, Vercel, Railway, LLM, email, etc.)** | $220-380/mo |\n| **Legal / Consulting** | $300-500 (one-time) |\n| **Total per month** | $220-380/mo |\n| **Total 8 weeks** | $~600-900 |\n\n**This is the cheapest possible MVP for a VC tool.**\n\n---\n\n## 6. RISK REGISTER\n\n### Top 10 Risks (Probability × Impact)\n\n| # | Risk | Description | Probability | Impact | Mitigation | Contingency |\n|---|------|-------------|-------------|--------|-----------|------------|\n| **1** | **Twitter API access denied or repriced** | Twitter API v2 free tier discontinued or access rejected. Cold start: no data. | High (30%) | Critical (10/10) | Apply for Basic tier ($100/mo) on Day 1. Maintain Apify scraper as fallback ($49/mo). Diversify sources (no single source >40% of leads). | Apify scraper becomes primary (less signal quality, +$49/mo cost). |\n| **2** | **LLM scoring accuracy <60%** | Prompt engineering fails. Founders scored are irrelevant to VC. Agent adds no value. | Medium (35%) | High (9/10) | Few-shot prompting: design partner provides 5 \"yes\" + 5 \"no\" examples. Ensemble scoring: score same founder 3 ways, average. Iterate prompt weekly. | Pivot to manual scoring (VC manually rates founders, agent learns). Or kill the agent, use existing data sources. |\n| **3** | **Cold emails land in spam** | Deliverability fails. Founders don't see emails. Reply rate 0-2%. Metrics collapse. | High (40%) | Critical (10/10) | Use Postmark (best deliverability). Warm sending domain for 2 weeks before scale. Keep initial volume <50/day. Personalize heavily (specific signals in email). Add unsubscribe link + CAN-SPAM headers. | Pause outreach, focus on inbound. Pivot to Twitter DM instead of email. |\n| **4** | **Design partner disengages mid-project** | VC partner loses interest, stops responding. Lose feedback loop. Build in vacuum. | Low (10%) | Critical (10/10) | Weekly demos with clear value. Ship something useful by Week 3. Have 2 backup VC contacts identified by Week 1. Consider building something useful even without a partner (use for own sourcing). | Switch to backup VC contact. Or pivot to operator tool (no VC required). |\n| **5** | **Founder email addresses not findable** | Twitter handles don't lead to emails. 50%+ of founders have no email in public. Outreach stalls. | Medium (45%) | High (8/10) | Integrate Hunter.io ($49/mo, finds emails from domain + name). Use Apollo.io as backup ($50/mo). Fallback to Twitter DM. Accept that 30-40% of founders won't have emails found. | Use Twitter as primary channel (DM instead of email). Or use CRunchbase emails where available. |\n| **6** | **Scope creep from design partner requests** | VC keeps asking for more features. \"Can you add LinkedIn?\" \"Can you do X?\" Distracted from P0. | High (50%) | Medium (7/10) | Strict P0/P1/P2 prioritization communicated upfront. Weekly scope review: is this P0? No? Backlog. No scope changes after Week 4 unless critical bug. | Have a pre-written \"here's what ships in v1 vs v2\" document. Stick to it. |\n| **7** | **One engineer burnout (if 1-person team)** | Unrealistic sprint targets. No weekends off. 12-hour days. Engineer burns out Week 5. Project stalls. | High (if 1 person) | Critical (9/10) | Realistic sprint targets (P0 only, aggressive but achievable). Mandatory no-weekend rule. Use off-the-shelf tools aggressively (Supabase, Inngest, shadcn/ui). Accept that testing will be light. | Bring in second person mid-project (Week 5+). Or cut scope (defer P1 stories). |\n| **8** | **CAN-SPAM / GDPR legal exposure** | Outreach emails lack unsubscribe link. EU founder data stored. Regulatory fine. | Low (5%) | Critical (10/10) | Add unsubscribe link to every email (Postmark does this automatically). Don't email EU founders without explicit opt-in (or add consent mechanism). Consult lawyer 1 hour ($300) Week 6. | Pause outreach to EU. Audit all emails for compliance. Implement unsubscribe feature. |\n| **9** | **Data quality: duplicate/stale founders** | Deduplication logic fails. Same founder emailed twice. Or founders with no signals in 2 months still in \"hot lead\" list. | High (40%) | Medium (6/10) | Fuzzy matching tuning + manual review of top duplicates. Auto-archive founders with no new signals in 30 days (soft delete, can restore). | Accept 5-10% duplicate rate as acceptable. Or add manual dedup step (human reviews all duplicates). |\n| **10** | **LLM costs exceed $200/mo budget** | More founders than expected. Scoring too expensive. Bills climb to $500+/mo. | Low (10%) | Medium (7/10) | Monitor daily LLM spend (Anthropic API dashboard). Use Haiku for bulk (cheap). Cache scores aggressively. Set hard budget caps with Anthropic (they'll alert if approaching). Estimate now: 200 founders × 500 tokens × $0.25/1M = $2.50/day = $75/mo (under budget). | Switch to GPT-4o-mini (slightly cheaper). Or reduce scoring frequency (score every 7 days instead of on every signal). |\n\n### Probability × Impact Matrix\n\n```\nCRITICAL (act immediately):\n- Risk 1 (Twitter API): 30% × 10 = 3.0\n- Risk 3 (Email spam): 40% × 10 = 4.0\n- Risk 4 (VC disengages): 10% × 10 = 1.0\n- Risk 8 (Legal): 5% × 10 = 0.5\n\nHIGH (plan & mitigate):\n- Risk 2 (Scoring accuracy): 35% × 9 = 3.15\n- Risk 5 (Email finding): 45% × 8 = 3.6\n- Risk 7 (Burnout): 50% × 9 = 4.5\n\nMEDIUM (watch):\n- Risk 6 (Scope creep): 50% × 7 = 3.5\n- Risk 9 (Data quality): 40% × 6 = 2.4\n- Risk 10 (LLM costs): 10% × 7 = 0.7\n```\n\n**Top priority mitigations (do immediately):**\n1. Apply for Twitter API access Day 1\n2. Set up Postmark + warm sending domain by Week 4\n3. Schedule weekly design partner demos\n4. Have lawyer review CAN-SPAM Week 6\n\n---\n\n## 7. GO-TO-MARKET FOR THE AGENT ITSELF\n\n### Validation Framework: 3 Phases\n\n#### Phase 1: Shadow Mode (Weeks 3-4)\n\n**Concept:** Agent runs in parallel with VC's existing manual process. No pressure. Compare outputs.\n\n**Approach:**\n1. VC does their normal sourcing (Twitter, newsletters, manual email drafting) for 2 weeks\n2. Simultaneously, agent sources founders (doesn't email them yet)\n3. Compare: Did agent find the same founders the VC found? Better ones?\n4. Metrics: \n   - Overlap: % of founders VC found that agent also found (target: >80%)\n   - Net-new: % of founders agent found that VC didn't (target: >20% net-new)\n   - Quality: of net-new founders, % that VC says \"I'd want to talk to this person\" (target: >50%)\n\n**Success bar:** Agent finds everything human finds + finds new valuable prospects.\n\n**If failing:** Adjust data sources (add LinkedIn), adjust scoring prompt, or accept that sourcing is too subjective (every VC different).\n\n---\n\n#### Phase 2: Assisted Mode (Weeks 5-6)\n\n**Concept:** Agent is primary sourcing tool. Operator reviews and approves all outreach. Measure time savings.\n\n**Approach:**\n1. VC opens dashboard daily (6 AM)\n2. Reviews top 50 founders (sorted by score)\n3. Approves emails for best-fit founders (20-30 emails/day)\n4. Operator edits drafts if needed, sends\n5. Track time: how long does this take? (baseline: 2-3 hours/day manual, target: 30 min/day agent-assisted)\n\n**Metrics:**\n- Time spent on sourcing: manual baseline vs. agent-assisted (target: 10+ hours/week saved)\n- Emails sent per day (target: 20-30)\n- Quality check: VC confirms >70% of sourced founders are \"yes, I'd email them\" (subjective, but good signal)\n\n**Success bar:** Agent is noticeably faster than manual. VC says \"I can't go back to doing this manually.\"\n\n**If failing:** \n- If time saved <5 hrs/week: process is too clunky. Redesign approval queue UX.\n- If quality <50%: scoring is bad. Rewrite prompt. Add more signals.\n- If VC still prefers manual: your problem statement might be wrong. They don't actually want automation. Pivot.\n\n---\n\n#### Phase 3: Autonomous Mode (Weeks 7-8)\n\n**Concept:** Agent runs fully. Operator reviews digest once/day. End-to-end measurement.\n\n**Approach:**\n1. Agent emails founders automatically (human-approved in Phase 2, now automated)\n2. Operator reviews replies and books meetings\n3. Track complete funnel: sourced → emailed → replied → meeting\n\n**Metrics (Primary KPIs):**\n- Reply rate: replies ÷ emails sent (target: >12%, baseline cold email: 5-8%)\n- Meetings booked: target 1-2 per week\n- Time saved: <30 min/day operator review (target: 10+ hrs/week)\n\n**Success bar:** \n- Green light: reply rate >12% + meetings >1/week + VC NPS >8 → **Scale to next VC**\n- Yellow light: reply rate 5-12% + scoring >70% → **Iterate 2 more weeks** (tweak email tone, add data sources)\n- Red light: reply rate <5% + scoring <60% + VC stops using → **Evaluate if fixable**\n\n---\n\n### Metrics to Track\n\n**Real-time Dashboard** (for operators):\n- Founders sourced (today, this week, all-time)\n- Emails sent (today, this week, all-time)\n- Open rate (realtime from Postmark)\n- Click rate (realtime from Postmark)\n- Reply count (manual, updated by operator)\n- Meetings booked (manual calendar check)\n\n**Weekly Report** (auto-generated):\n- Sourcing summary: 50 new founders, avg score 75, top score 94\n- Outreach summary: 40 emails sent, 18 opened (45%), 5 replied (12%)\n- Engagement: 1 meeting booked this week\n- Time saved: estimated 8 hours (manual baseline 15 hrs/week, agent 7 hrs/week)\n\n**Monthly Metrics** (for decision-making):\n- Sourcing efficiency: founders per hour spent (target: 10+ founders/hour)\n- Quality: % of sourced founders that VC emailed (target: >70%)\n- Conversion: sourced → meeting (target: 2-5%)\n- ROI: meetings booked ÷ LLM spend (target: >5 meetings per $100 LLM spend)\n\n---\n\n### When to Know It's Working\n\n#### Green Light (Continue & Scale)\n\n**Triggers (any one is sufficient):**\n1. Reply rate >15% on 50+ emails sent\n2. VC books 2+ meetings per week from agent sourcing\n3. VC says \"I can't imagine doing sourcing without this\"\n4. Time saved >10 hours/week verified by VC\n5. NPS >8/10 on \"would you recommend this to another VC?\"\n\n**Actions:**\n- Plan v1.1 (Weeks 9-10): multi-tenant, self-service onboarding\n- Reach out to 5-10 similar VCs: \"We built a sourcing agent. Interested in beta?\"\n- Optimize for expansion: landing page, demo video, case study with design partner\n\n---\n\n#### Yellow Light (Iterate)\n\n**Triggers (all present):**\n1. Reply rate 5-12% on 50+ emails sent\n2. Scoring accuracy 60-70% (VC confirms relevance but not perfect)\n3. Time saved 5-10 hours/week\n4. VC still uses system but \"needs tweaks\"\n\n**Actions:**\n- Spend Weeks 9-10 entirely on improvement:\n  - A/B test email templates (3 variants, measure reply rate)\n  - Add LinkedIn source (if not already done)\n  - Tune scoring prompt (re-run on past 100 founders, improve accuracy)\n  - Expand deduplication (catch more duplicates)\n- Re-measure Week 11: did we hit >12% reply rate?\n- If yes: go to Scale. If no: go to Red light.\n\n---\n\n#### Red Light (Pivot or Kill)\n\n**Triggers (at least 2 present):**\n1. Reply rate <5% on 50+ emails sent\n2. Scoring accuracy <60% (VC says \"most are irrelevant\")\n3. Time saved <3 hours/week (not worth using)\n4. VC stops using by Week 8 (\"This isn't helping. Going back to manual.\")\n\n**Actions (choose one):**\n1. **Pivot to different angle:** Maybe cold email isn't the answer. Try:\n   - Inbound (alert founders to VC, not the other way)\n   - Warm introductions (find partners to introduce)\n   - Community (host events, source from attendees)\n   - Resume scanning (accept term sheets, parse resumes)\n\n2. **Pivot to different customer:** Maybe founders aren't the right persona. Try:\n   - Corporate venture arms (source partners for licensing deals)\n   - Accelerators (source startups to invest in batches)\n   - Emerging manager programs (source founders becoming angels/VCs)\n\n3. **Kill:** If core assumption is wrong (\"VCs don't actually want sourcing automation\"), stop. Move to next idea.\n\n---\n\n### Iteration Strategy (Weekly Cadence)\n\n**Monday (2 hrs):**\n- Review last week's metrics\n- Calculate reply rate, reply quality (VC feedback on replies received)\n- Identify bottleneck: inaccurate sourcing? Bad emails? Low volume?\n\n**Tuesday (3 hrs):**\n- Prompt tuning: based on false positives/negatives from Monday, rewrite scoring or email prompt\n- Test on past 20 founders: does new version score better?\n- A/B test email tone: variant A (current) vs. variant B (new), measure on next 20 emails\n\n**Wednesday-Thursday (4 hrs):**\n- Ship improvements: commit new prompts, deploy\n- Optional: add new data source (HackerNews, if others are optimized)\n\n**Friday (2 hrs):**\n- Design partner check-in: demo improvements, collect feedback\n- Adjust priorities based on feedback\n\n**Expected outcome:** Reply rate increases 1-2% per week (from 8% → 10% → 12% → 14%) through iterative refinement.\n\n---\n\n### Validation Playbook\n\n**Week 1-2 Checkpoint:**\n- Founders appearing in database? YES/NO\n- Design partner confirms thesis is understood? YES/NO\n- If NO on either: stop, fix before continuing\n\n**Week 3 Checkpoint:**\n- Scoring accuracy >60%? YES/NO\n- If NO: iterate prompt. If YES after iteration: continue\n\n**Week 4 Checkpoint:**\n- 5+ real emails sent without bouncing? YES/NO\n- Email draft acceptance >40%? YES/NO\n- If NO: tweak email tone + personalization\n\n**Week 5 Checkpoint:**\n- Pipeline runs 3 days without manual intervention? YES/NO\n- If NO: fix jobs/monitoring\n\n**Week 6 Checkpoint:**\n- 50+ emails sent with >10% reply rate? YES/NO\n- If <5% reply rate: email quality problem. If >10% but <50 emails sent: volume problem.\n\n**Week 8 Checkpoint (Go/No-Go):**\n- Reply rate + meetings + time saved + NPS → decision matrix\n- Commit to Scale, Iterate, or Pivot\n\n---\n\n## 8. COMPLETE TECH STACK SUMMARY\n\n### Stack at a Glance\n\n```\nFRONTEND: Next.js (Vercel) + React + shadcn/ui\nBACKEND: Python + FastAPI (Railway)\nDATABASE: PostgreSQL (Supabase)\nLLM: Claude (Haiku + Sonnet)\nEMAIL: Postmark\nJOBS: Inngest\nCRM: Custom (Supabase tables)\nMONITORING: Sentry + PostHog + Slack\nDATA SOURCES: Twitter API v2, Product Hunt RSS, Crunchbase API\n```\n\n### Deployment Architecture\n\n```\n┌─────────────────────────────────────────────────────────────┐\n│                        User (VC Operator)                    │\n└──────┬──────────────────────────────────────────────────────┘\n       │\n       ├─── HTTPS ──→ Vercel (Next.js Dashboard)\n       │              ├── /dashboard (founder list)\n       │              ├── /founder/[id] (detail)\n       │              ├── /outreach/queue (approval)\n       │              └── /analytics (metrics)\n       │\n       └─── API ──→ Railway (FastAPI Backend)\n                    ├── POST /api/approve-email\n                    ├── GET /api/founders\n                    ├── POST /api/update-founder-state\n                    └── (all data from Supabase)\n\n┌──────────────────────────────────────────────────────────────┐\n│                  Scheduled Jobs (Inngest)                    │\n├──────────────────────────────────────────────────────────────┤\n│ 6 AM Daily: Twitter Ingest → PH Scrape → Dedup → Score      │\n│            → Generate Drafts → Notify Operator                │\n└──────────────────────────────────────────────────────────────┘\n\n┌──────────────────────────────────────────────────────────────┐\n│                  External Integrations                        │\n├──────────────────────────────────────────────────────────────┤\n│ Twitter API v2 (keyword monitoring)                          │\n│ Product Hunt (RSS feed)                                      │\n│ Crunchbase (company enrichment)                              │\n│ Anthropic Claude (scoring + email generation)                │\n│ Postmark (email sending + webhooks)                          │\n│ Hunter.io (email finding)                                    │\n│ Sentry (error tracking)                                      │\n│ PostHog (analytics)                                          │\n│ Slack (pipeline alerts)                                      │\n└──────────────────────────────────────────────────────────────┘\n```\n\n### Monthly Cost Breakdown\n\n| Component | Cost | Notes |\n|-----------|------|-------|\n| **Supabase (PostgreSQL)** | $0-25 | Free tier: 500MB + 100K requests/mo. Upgrade to paid ($25) if exceeding. |\n| **Vercel (Next.js)** | Free | Free tier: 100GB bandwidth, 12 serverless function hours. Sufficient for MVP. |\n| **Railway (FastAPI backend)** | $5-20 | Free tier: $5 credits/mo. Additional compute $0.17/hour. Estimate 10-50 hours usage/mo. |\n| **Postmark (Email)** | $15 | 10K emails/mo tier. MVP sends ~50-100/week, well under limit. |\n| **Claude API (LLM)** | $75-150 | Haiku $0.25/1M tokens (bulk scoring), Sonnet $3/1M (email generation). Estimate $2-5/day. |\n| **Twitter API v2 (Basic)** | $100 | Monthly subscription. Required for reliable access (free tier has limits). |\n| **Hunter.io (Email API)** | $49 | 5K lookups/month. Free tier: 50/month (insufficient). |\n| **Inngest (Jobs)** | Free | 10K function invocations/month free tier. MVP runs ~10 jobs/day, 300/month. |\n| **Crunchbase (Enrichment)** | Free | Free API tier (rate-limited but sufficient). |\n| **Sentry (Monitoring)** | Free | Free tier: 5K errors/month. Sufficient for MVP. |\n| **PostHog (Analytics)** | Free | Free tier: all events tracked. Self-hosted or cloud. |\n| **Slack (Alerts)** | Free | Free tier: bots welcome. |\n| **Domain + DNS** | $12/yr | Registrar fee. Minimal. |\n| **TOTAL MONTHLY** | **$256-380** | Ranges: free tier utilization to paid tier if usage exceeds. |\n\n**Assumption:** This is sustainable for 8 weeks with 1-2 person team. At scale (10+ VCs), costs would increase linearly (Supabase $100+, LLM $500+, email $100+) but still <$1K/mo at 10 VCs.\n\n---\n\n## 9. HONEST ASSESSMENT & COMMON PITFALLS\n\n### What Will Go Wrong\n\n1. **Scoring is harder than expected:** Founders are diverse. \"Series A SaaS in fintech\" is easy. \"Pre-seed fintech founder with technical background but no revenue\" is hard. You will probably end up with 60-70% accuracy at launch, not 90%. That's okay. Iterate.\n\n2. **Email quality is a bottleneck:** Even with Claude Sonnet, email drafts sound generic. The signal quality matters (specific Twitter quote vs. generic \"saw your PH launch\"). Expect to iterate email prompt 5-10 times.\n\n3. **Email deliverability is unpredictable:** Even with Postmark, some emails land in spam. Test sending 10 emails to your own company + test accounts first. Warm up the domain. Expect 2-5% bounces initially.\n\n4. **Founder data is messy:** Twitter handles don't always have real names. Company names vary (acme vs. acme.io vs. acme inc). Email addresses are hidden. You will spend more time on data quality than expected.\n\n5. **Design partner feedback is slow:** VCs are busy. Getting weekly check-ins is a win. Build feedback loops that don't require them (metrics dashboard they can review asynchronously).\n\n6. **One person will get overwhelmed:** If it's a 1-person team, sprints of \"score 200 founders by Friday\" are brutal. Build in buffer time. Sanity > perfection.\n\n7. **Scope creep is real:** Design partner will ask for LinkedIn, Slack integration, portfolio company matching, etc. Say \"v2\" often. Have a pre-made \"here's what's in v1\" doc.\n\n### Biggest Risks to Avoid\n\n| Don't Do | Why | What to Do Instead |\n|----------|-----|-------------------|\n| Build a custom LLM or fine-tune Claude | Massive complexity, no advantage over prompting | Invest in prompt engineering. Few-shot examples are your friend. |\n| Use Airtable as primary database | Record limits will strangle you by Week 2 | Use Postgres from day 1. Accept the extra 10 hours of setup. |\n| Try to support LinkedIn immediately | API restrictions, rate limits, legal complexity | Defer to v1.5. Twitter + PH are plenty for MVP. |\n| Build multi-tenant SaaS in Week 1 | Adds 4+ weeks of complexity you don't need yet | Single-tenant for design partner only. Multi-tenant post-MVP. |\n| Autonomous sending without human approval | Legal risk, brand risk if email quality is bad | Week 1-7: human-in-the-loop. Week 8+: consider autonomous. |\n| Neglect data quality | Garbage in = garbage out. Bad scoring = bad product | Spend time on dedup, signal enrichment, data validation. |\n| Wait until Week 8 to involve design partner | Build in vacuum, ship wrong thing | Weekly demos from Week 2 onwards. |\n| Under-estimate email finding | Assume all founders have public emails. They don't. | Budget for Hunter.io from Week 1. Accept 30% of founders have no email. |\n\n### If You Run Out of Time\n\n**Priority order (cut from the bottom):**\n\n1. P0 Stories: ship these no matter what\n2. P1 Stories: ship as many as you can\n3. Testing: light happy-path tests (full test suite is nice-to-have)\n4. Documentation: playbook + API reference (polish can wait)\n5. UI Polish: functional > pretty (shadcn/ui is good enough)\n6. Analytics: basic dashboards (advanced charts are v2)\n7. Multi-VC support: focus on one design partner (scale after MVP validation)\n8. Advanced features: LinkedIn, Slack, drippy campaigns → all v2\n\nIf you're running behind Week 6, cut P1 stories. Ship P0 + a handful of P1.\n\n---\n\n## APPENDIX: Sample Sprint Checklist (Week 1)\n\n```\nWEEK 1 DELIVERABLES CHECKLIST\n\n🏗️ Infrastructure\n  ☐ GitHub repo created with monorepo structure (/apps/dashboard, /apps/backend)\n  ☐ CI/CD pipeline set up (GitHub Actions)\n  ☐ Supabase project created, database initialized\n  ☐ .env file with secrets (Twitter API, Claude API, Postmark key) -- NOT committed\n  ☐ Local dev environment working (Docker or direct setup)\n  ☐ Sentry project created for error tracking\n  ☐ Slack workspace integrated for alerts\n\n📊 Database\n  ☐ Schema designed: founders, signals, thesis_configs, outreach, emails tables\n  ☐ Migrations in place (SQL or ORM)\n  ☐ Indexes created for performance (founder_id, timestamp, score)\n  ☐ RLS policies set up (Supabase Auth integration)\n\n🐦 Twitter Ingestion\n  ☐ Twitter API v2 Basic tier access approved\n  ☐ Python script written: fetch tweets by keywords + from accounts\n  ☐ Deduplication logic: fuzzy match on author_handle + date\n  ☐ Test run: 500+ tweets ingested into database\n  ☐ Logs: zero API errors, processing time <5 minutes\n  ☐ Cron job scheduled (6 AM daily)\n\n🌟 Product Hunt Ingestion\n  ☐ RSS feed parser written\n  ☐ Daily scrape extracts: product name, maker, tagline, upvotes, URL\n  ☐ Test run: 10-20 launches ingested per day\n  ☐ Data stored in signals table with source='product_hunt'\n\n🔄 Deduplication\n  ☐ Fuzzy matching function: name_similarity + company_similarity\n  ☐ Test on 50 founders: zero false negatives (missed duplicates), <5% false positives\n  ☐ Merge logic: combine signals into single founder record\n\n📈 Dashboard Skeleton\n  ☐ Next.js app created with create-next-app\n  ☐ Supabase Auth integration (magic link or GitHub OAuth)\n  ☐ `/dashboard` page: lists founders, fetches from Supabase API\n  ☐ Basic styling (HTML + Tailwind, no components yet)\n  ☐ Load time: <3 seconds\n\n🤝 Design Partner Engagement\n  ☐ Initial meeting scheduled to understand thesis\n  ☐ Design partner provides investment thesis (written form)\n  ☐ Confirm data quality on 20 founders (do they look real?)\n\n✅ Code Quality\n  ☐ All code committed to Git, no secrets\n  ☐ CI passing (linting, basic tests if added)\n  ☐ README updated with setup instructions\n\nWEEK 1 SUCCESS METRICS:\n  ✓ 100+ real founders in database\n  ✓ Zero data corruption\n  ✓ Pipeline runs without manual intervention\n  ✓ Design partner confirms data quality acceptable\n  ✓ Team understands data flow, no major blockers identified\n```\n\n---\n\n## FINAL RECOMMENDATION\n\n**Build the MVP exactly as described in this plan with a 2-person team over 8 weeks.**\n\n**Success looks like:**\n- Week 3: Design partner says \"that's accurate\" on scoring\n- Week 4: First 5 real emails sent to actual founders\n- Week 6: 50+ emails sent, replies coming in, pipeline running autonomously\n- Week 8: Reply rate >12%, 2-3 meetings booked, design partner says \"I can't imagine doing sourcing without this\"\n\n**Go/no-go decision Week 8:**\n- Green: Scale to 5 VCs, build v1.1 (multi-tenant)\n- Yellow: Iterate 2 weeks, then decide\n- Red: Learn from failure, pivot to different problem\n\n**Key to success:**\n1. Design partner relationship (weekly check-ins, feedback)\n2. Data quality (dedup, signal enrichment, validation)\n3. LLM prompt engineering (iterate weekly)\n4. Email personalization (specific signals, not generic)\n5. Realistic scope (P0 only, ship not perfect)\n6. Team morale (no crunch, one person can do it but tight)\n\nYou have a solvable problem, a clear path, and a small budget that works. Execute this plan and you'll have a working agent by Week 8."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":10770,"cache_read_input_tokens":48757,"cache_creation":{"ephemeral_5m_input_tokens":0,"ephemeral_1h_input_tokens":10770},"output_tokens":1,"service_tier":"standard","inference_geo":"not_available"}},"requestId":"req_011CY8xBiwg77Bm3eCjDvUTP","type":"assistant","uuid":"fe23f128-7a93-4862-9b98-34f32693be59","timestamp":"2026-02-15T01:06:51.196Z"}
