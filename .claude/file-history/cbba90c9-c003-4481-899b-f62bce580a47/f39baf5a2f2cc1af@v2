"""
YOLO + ByteTrack Player Tracking Pipeline

Provides stable track_ids across video frames for player isolation.
Uses YOLOv8 for person detection and ByteTrack for tracking.
"""

from dataclasses import dataclass, asdict, field
from typing import Optional
import base64
import cv2
import numpy as np

try:
    from ultralytics import YOLO
    import supervision as sv
    TRACKING_AVAILABLE = True
except ImportError:
    TRACKING_AVAILABLE = False
    print("[WARN] ultralytics or supervision not installed - tracking disabled")

try:
    import mediapipe as mp
    MEDIAPIPE_AVAILABLE = True
except ImportError:
    MEDIAPIPE_AVAILABLE = False
    print("[WARN] mediapipe not installed - pose estimation disabled")


@dataclass
class TrackedPerson:
    """A single tracked person in a frame."""
    track_id: int
    bbox: tuple  # (x, y, w, h) normalized 0-1
    landmarks: Optional[list] = None  # 33 MediaPipe landmarks
    confidence: float = 0.0
    pose_confidence: float = 0.0


@dataclass
class FrameTracking:
    """Tracking data for a single frame."""
    frame_index: int
    timestamp: float
    tracks: list = field(default_factory=list)  # List of TrackedPerson


@dataclass
class TrackSummary:
    """Summary info for a unique track_id."""
    track_id: int
    first_frame: int
    last_frame: int
    frame_count: int
    avg_confidence: float
    thumbnail_base64: str = ""
    side: Optional[str] = None  # 'near' or 'far'


class PlayerTracker:
    """
    YOLO + ByteTrack player tracking pipeline.

    Provides stable track_ids that persist across frames,
    enabling strict player isolation for downstream analysis.
    """

    def __init__(self, fps: int = 30, model_path: str = "yolov8n.pt"):
        """
        Initialize tracker with YOLO model and ByteTrack.

        Args:
            fps: Video frame rate (used for ByteTrack timing)
            model_path: Path to YOLO model weights
        """
        if not TRACKING_AVAILABLE:
            raise RuntimeError("Tracking requires ultralytics and supervision packages")

        self.fps = fps
        self.yolo = YOLO(model_path)
        self.byte_tracker = sv.ByteTrack(
            track_activation_threshold=0.25,
            lost_track_buffer=30,
            minimum_matching_threshold=0.8,
            frame_rate=fps
        )

        # Initialize MediaPipe Pose for skeleton extraction
        self.pose = None
        if MEDIAPIPE_AVAILABLE:
            self.pose = mp.solutions.pose.Pose(
                static_image_mode=True,
                model_complexity=1,
                min_detection_confidence=0.5,
                min_tracking_confidence=0.5
            )

    def process_video(self, video_path: str, skip_frames: int = 0) -> list:
        """
        Process entire video through YOLO + ByteTrack + MediaPipe pipeline.

        Args:
            video_path: Path to input video file
            skip_frames: Process every Nth frame (0 = process all)

        Returns:
            List of FrameTracking objects with stable track_ids
        """
        results = []
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            raise ValueError(f"Cannot open video: {video_path}")

        # Get video properties
        actual_fps = cap.get(cv2.CAP_PROP_FPS) or self.fps
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

        frame_index = 0
        processed_count = 0

        print(f"[Tracking] Processing {total_frames} frames at {actual_fps:.1f} fps")

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # Optional frame skipping for performance
            if skip_frames > 0 and frame_index % (skip_frames + 1) != 0:
                frame_index += 1
                continue

            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            h, w = frame.shape[:2]

            # 1. YOLO detection (persons only, class_id=0)
            detections = self.yolo(frame, classes=[0], verbose=False)[0]
            sv_detections = sv.Detections.from_ultralytics(detections)

            # 2. ByteTrack assigns stable track_ids
            tracked = self.byte_tracker.update_with_detections(sv_detections)

            # 3. Process each tracked person
            frame_tracks = []

            if tracked.tracker_id is not None and len(tracked.tracker_id) > 0:
                for i, track_id in enumerate(tracked.tracker_id):
                    bbox_xyxy = tracked.xyxy[i]  # x1, y1, x2, y2
                    confidence = float(tracked.confidence[i])

                    # Add padding to bbox for better pose estimation
                    pad = 0.1
                    x1, y1, x2, y2 = bbox_xyxy
                    bw, bh = x2 - x1, y2 - y1
                    x1_padded = max(0, x1 - bw * pad)
                    y1_padded = max(0, y1 - bh * pad)
                    x2_padded = min(w, x2 + bw * pad)
                    y2_padded = min(h, y2 + bh * pad)

                    # Crop ROI for MediaPipe
                    cropped = frame_rgb[int(y1_padded):int(y2_padded),
                                       int(x1_padded):int(x2_padded)]

                    # 4. Run MediaPipe Pose on cropped region
                    landmarks_full = None
                    pose_confidence = 0.0

                    if self.pose is not None and cropped.size > 0:
                        pose_result = self.pose.process(cropped)

                        if pose_result.pose_landmarks:
                            # Map landmarks back to full frame coordinates
                            landmarks_full = []
                            visibility_sum = 0.0

                            for lm in pose_result.pose_landmarks.landmark:
                                # Transform from crop coordinates to full frame (normalized)
                                full_x = (x1_padded + lm.x * (x2_padded - x1_padded)) / w
                                full_y = (y1_padded + lm.y * (y2_padded - y1_padded)) / h

                                landmarks_full.append({
                                    'x': full_x,
                                    'y': full_y,
                                    'z': lm.z,
                                    'visibility': lm.visibility
                                })
                                visibility_sum += lm.visibility

                            pose_confidence = visibility_sum / len(landmarks_full)

                    # Normalize bbox to 0-1 range
                    norm_bbox = (
                        x1_padded / w,
                        y1_padded / h,
                        (x2_padded - x1_padded) / w,
                        (y2_padded - y1_padded) / h
                    )

                    frame_tracks.append(TrackedPerson(
                        track_id=int(track_id),
                        bbox=norm_bbox,
                        landmarks=landmarks_full,
                        confidence=confidence,
                        pose_confidence=pose_confidence
                    ))

            results.append(FrameTracking(
                frame_index=frame_index,
                timestamp=frame_index / actual_fps,
                tracks=frame_tracks
            ))

            frame_index += 1
            processed_count += 1

            # Progress logging
            if processed_count % 100 == 0:
                print(f"[Tracking] Processed {processed_count} frames...")

        cap.release()
        print(f"[Tracking] Complete: {processed_count} frames, "
              f"{len(self._get_unique_track_ids(results))} unique tracks")

        return results

    def _get_unique_track_ids(self, results: list) -> set:
        """Get set of all unique track IDs from results."""
        track_ids = set()
        for frame in results:
            for track in frame.tracks:
                track_ids.add(track.track_id)
        return track_ids

    def extract_track_thumbnails(
        self,
        video_path: str,
        tracking_results: list,
        size: tuple = (80, 120)
    ) -> dict:
        """
        Extract thumbnail image for each unique track_id.

        Args:
            video_path: Path to input video
            tracking_results: List of FrameTracking from process_video
            size: Thumbnail dimensions (width, height)

        Returns:
            Dict mapping track_id to base64-encoded JPEG thumbnail
        """
        thumbnails = {}
        cap = cv2.VideoCapture(video_path)

        if not cap.isOpened():
            return thumbnails

        h_vid = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        w_vid = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))

        # Find first clear appearance of each track (confidence > 0.7)
        track_first_frame = {}
        for frame in tracking_results:
            for track in frame.tracks:
                if track.track_id not in track_first_frame:
                    if track.confidence > 0.7:
                        track_first_frame[track.track_id] = (
                            frame.frame_index,
                            track.bbox
                        )

        # Extract thumbnails
        for track_id, (frame_idx, bbox) in track_first_frame.items():
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()

            if ret:
                x, y, bw, bh = bbox
                x1, y1 = int(x * w_vid), int(y * h_vid)
                x2, y2 = int((x + bw) * w_vid), int((y + bh) * h_vid)

                # Ensure bounds are valid
                x1, y1 = max(0, x1), max(0, y1)
                x2, y2 = min(w_vid, x2), min(h_vid, y2)

                if x2 > x1 and y2 > y1:
                    crop = frame[y1:y2, x1:x2]
                    crop = cv2.resize(crop, size)
                    _, buffer = cv2.imencode('.jpg', crop,
                                            [cv2.IMWRITE_JPEG_QUALITY, 85])
                    thumbnails[track_id] = base64.b64encode(buffer).decode('utf-8')

        cap.release()
        return thumbnails

    def get_track_summaries(self, tracking_results: list) -> list:
        """
        Generate summary statistics for each unique track.

        Args:
            tracking_results: List of FrameTracking from process_video

        Returns:
            List of TrackSummary objects
        """
        track_data = {}

        for frame in tracking_results:
            for track in frame.tracks:
                tid = track.track_id

                if tid not in track_data:
                    track_data[tid] = {
                        'first_frame': frame.frame_index,
                        'last_frame': frame.frame_index,
                        'frame_count': 0,
                        'confidence_sum': 0.0,
                        'avg_y': 0.0  # For side detection
                    }

                data = track_data[tid]
                data['last_frame'] = frame.frame_index
                data['frame_count'] += 1
                data['confidence_sum'] += track.confidence
                data['avg_y'] += track.bbox[1]  # y position

        # Build summaries
        summaries = []
        for tid, data in track_data.items():
            avg_y = data['avg_y'] / data['frame_count'] if data['frame_count'] > 0 else 0.5

            summaries.append(TrackSummary(
                track_id=tid,
                first_frame=data['first_frame'],
                last_frame=data['last_frame'],
                frame_count=data['frame_count'],
                avg_confidence=data['confidence_sum'] / data['frame_count'],
                side='near' if avg_y > 0.5 else 'far'
            ))

        # Sort by frame count (most visible first)
        summaries.sort(key=lambda s: -s.frame_count)

        return summaries

    def close(self):
        """Release MediaPipe resources."""
        if self.pose is not None:
            self.pose.close()


def extract_unique_tracks(tracking_results: list) -> list:
    """
    Extract unique track IDs with metadata (standalone function).

    Args:
        tracking_results: List of FrameTracking objects

    Returns:
        List of dicts with track_id, first_seen, confidence
    """
    tracks = {}

    for frame in tracking_results:
        for track in frame.tracks:
            if track.track_id not in tracks:
                tracks[track.track_id] = {
                    'track_id': track.track_id,
                    'first_seen': frame.timestamp,
                    'confidence': track.confidence
                }
            else:
                # Update to max confidence seen
                tracks[track.track_id]['confidence'] = max(
                    tracks[track.track_id]['confidence'],
                    track.confidence
                )

    return list(tracks.values())


def serialize_tracking_results(tracking_results: list) -> list:
    """
    Convert tracking results to JSON-serializable format.

    Args:
        tracking_results: List of FrameTracking objects

    Returns:
        List of dicts suitable for JSON serialization
    """
    serialized = []

    for frame in tracking_results:
        frame_data = {
            'frame_index': frame.frame_index,
            'timestamp': frame.timestamp,
            'tracks': []
        }

        for track in frame.tracks:
            track_data = {
                'track_id': track.track_id,
                'bbox': {
                    'x': track.bbox[0],
                    'y': track.bbox[1],
                    'w': track.bbox[2],
                    'h': track.bbox[3]
                },
                'confidence': track.confidence,
                'pose_confidence': track.pose_confidence,
                'landmarks': track.landmarks
            }
            frame_data['tracks'].append(track_data)

        serialized.append(frame_data)

    return serialized


# Fallback tracker for when YOLO/ByteTrack not available
class SimpleBboxTracker:
    """
    Simple bbox proximity tracker as fallback.
    Uses center-of-mass distance for matching.
    """

    def __init__(self, threshold: float = 0.15):
        self.threshold = threshold
        self.next_id = 1
        self.tracks = {}  # track_id -> last bbox

    def update(self, bboxes: list) -> list:
        """
        Update tracker with new bboxes.

        Args:
            bboxes: List of (x, y, w, h) normalized bboxes

        Returns:
            List of track_ids corresponding to input bboxes
        """
        if not bboxes:
            return []

        assigned_ids = []
        used_tracks = set()

        for bbox in bboxes:
            best_id = None
            best_dist = float('inf')

            # Find closest existing track
            cx, cy = bbox[0] + bbox[2]/2, bbox[1] + bbox[3]/2

            for tid, last_bbox in self.tracks.items():
                if tid in used_tracks:
                    continue

                lcx, lcy = last_bbox[0] + last_bbox[2]/2, last_bbox[1] + last_bbox[3]/2
                dist = ((cx - lcx)**2 + (cy - lcy)**2) ** 0.5

                if dist < best_dist and dist < self.threshold:
                    best_dist = dist
                    best_id = tid

            if best_id is not None:
                assigned_ids.append(best_id)
                used_tracks.add(best_id)
                self.tracks[best_id] = bbox
            else:
                # New track
                assigned_ids.append(self.next_id)
                self.tracks[self.next_id] = bbox
                self.next_id += 1

        return assigned_ids
